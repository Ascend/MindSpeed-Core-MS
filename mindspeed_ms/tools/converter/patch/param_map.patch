diff --git a/megatron/core/distributed/distributed_data_parallel.py b/megatron/core/distributed/distributed_data_parallel.py
index 13e321f5..09dde0d5 100644
--- a/megatron/core/distributed/distributed_data_parallel.py
+++ b/megatron/core/distributed/distributed_data_parallel.py
@@ -10,6 +10,38 @@ from ..transformer.module import MegatronModule
 from ..transformer.transformer_config import TransformerConfig
 from .param_and_grad_buffer import ParamAndGradBuffer

+import os
+import json
+
+def save_param_map(buffers, save_path):
+    '''save param map'''
+    save_path = os.path.join(save_path, "param_map")
+
+    dp = parallel_state.get_data_parallel_rank(with_context_parallel=True)
+    pp = parallel_state.get_pipeline_model_parallel_rank()
+    tp = parallel_state.get_tensor_model_parallel_rank()
+    if not os.path.exists(save_path):
+        os.makedirs(save_path, exist_ok=True)
+    for buffer_id, buffer in enumerate(buffers):
+        files = [f for f in os.listdir(save_path) if f.startswith(f"param_map_buffer{buffer_id}_dp{dp}tp{tp}pp{pp}vpp")]
+        if not files:
+            vpp = 0
+            map_path = os.path.join(save_path, f"param_map_buffer{buffer_id}_dp{dp}tp{tp}pp{pp}vpp{vpp}.json")
+        else:
+            vpp_values = []
+            for f in files:
+                f_parts = f.split(".")
+                vpp = int(f_parts[0].split("vpp")[-1])
+                vpp_values.append(vpp)
+            cur_vpp = max(vpp_values) + 1
+            map_path = os.path.join(save_path, f"param_map_buffer{buffer_id}_dp{dp}tp{tp}pp{pp}vpp{cur_vpp}.json")
+        print(f"map_path: {map_path}: {buffer.param_index_map_full}")
+        try:
+            with open(map_path, "w") as outfile:
+                json.dump(buffer.param_index_map_full, outfile)
+        except Exception as e:
+            print(f' > WARNING: could not save param map files. \n{e}')
+

 class DistributedDataParallel(MegatronModule):
     """
@@ -39,6 +71,7 @@ class DistributedDataParallel(MegatronModule):

     def __init__(
         self,
+        save_path,
         config: TransformerConfig,
         module: torch.nn.Module,
         data_parallel_group: torch.distributed.ProcessGroup,
@@ -138,7 +171,7 @@ class DistributedDataParallel(MegatronModule):
             data_parallel_group,
             gradient_scaling_factor=1.0 / data_parallel_world_size,
         )
-
+        save_param_map(self.buffers, save_path)
         # Allocate separate param+grad buffers for expert parallel params' grads.
         self.expert_parallel_buffers = allocate_buffers_for_parameters(
             expert_parallel_params,
diff --git a/megatron/core/distributed/param_and_grad_buffer.py b/megatron/core/distributed/param_and_grad_buffer.py
index 8032591a..ed2799af 100644
--- a/megatron/core/distributed/param_and_grad_buffer.py
+++ b/megatron/core/distributed/param_and_grad_buffer.py
@@ -215,7 +215,7 @@ class ParamAndGradBuffer:
         gradient_scaling_factor: float,
         check_for_nan_in_grad: bool,
     ):
-
+        self.param_index_map_full = {}
         # Check that params are unique.
         unique_params = set()
         for param in params:
@@ -305,7 +305,14 @@ class ParamAndGradBuffer:
                     # data_start_index should already be padded.
                     assert data_start_index % self.data_parallel_world_size == 0
                 _create_new_bucket(data_start_index)
-
+            param_name = param_to_name[param]
+            self.param_index_map_full[param_name] = (
+                str(param.dtype),
+                param.shape,
+                data_start_index,
+                data_end_index,
+                bucket_id,
+            )
             self.param_index_map[param] = (
                 data_start_index,
                 data_end_index,
diff --git a/megatron/training/training.py b/megatron/training/training.py
index b1b5c668..966ee6c9 100644
--- a/megatron/training/training.py
+++ b/megatron/training/training.py
@@ -410,7 +410,8 @@ def get_model(model_provider_func, model_type=ModelType.encoder_or_decoder, wrap

     if wrap_with_ddp:
         config = get_model_config(model[0])
-        model = [DDP(config,
+        model = [DDP(args.save,
+                     config,
                      model_chunk,
                      data_parallel_group=mpu.get_data_parallel_group(with_context_parallel=True),
                      expert_data_parallel_group=mpu.get_data_modulo_expert_parallel_group(),
