diff --git a/torch.py b/torch.py
index 48532ea..b7335c5 100644
--- a/torch.py
+++ b/torch.py
@@ -10,7 +10,8 @@ from safetensors import deserialize, safe_open, serialize, serialize_file
 
 def storage_ptr(tensor: torch.Tensor) -> int:
     try:
-        return tensor.untyped_storage().data_ptr()
+        #return tensor.untyped_storage().data_ptr()
+        return tensor
     except Exception:
         # Fallback for torch==1.10
         try:
@@ -30,7 +31,8 @@ def _end_ptr(tensor: torch.Tensor) -> int:
 
 def storage_size(tensor: torch.Tensor) -> int:
     try:
-        return tensor.untyped_storage().nbytes()
+        return tensor.nbytes
+        #return tensor.untyped_storage().nbytes()
     except AttributeError:
         # Fallback for torch==1.10
         try:
@@ -67,9 +69,14 @@ def _filter_shared_not_shared(tensors: List[Set[str]], state_dict: Dict[str, tor
 
 
 def _find_shared_tensors(state_dict: Dict[str, torch.Tensor]) -> List[Set[str]]:
+    import numpy
     tensors = defaultdict(set)
+    tmp = torch.device("meta")
     for k, v in state_dict.items():
-        if v.device != torch.device("meta") and storage_ptr(v) != 0 and storage_size(v) != 0:
+       storage_val = storage_ptr(v).asnumpy() if isinstance(storage_ptr(v), numpy.ndarray) else numpy.array(storage_ptr(v))
+       storage_val1 = storage_size(v).asnumpy() if isinstance(storage_size(v), numpy.ndarray) else numpy.array(storage_size(v))
+       if v.device != torch.device("meta") and (storage_val != 0).all() and (storage_val1 != 0).all():
+       #if v.device != torch.device("meta") and storage_ptr(v) != 0 and storage_size(v) != 0:
             # Need to add device as key because of multiple GPU.
             tensors[(v.device, storage_ptr(v), storage_size(v))].add(k)
     tensors = list(sorted(tensors.values()))
@@ -283,7 +290,8 @@ def save_file(
     save_file(tensors, "model.safetensors")
     ```
     """
-    serialize_file(_flatten(tensors), filename, metadata=metadata)
+    torch.serialization.safe_save_file(tensors, filename, metadata=metadata)
+    # serialize_file(_flatten(tensors), filename, metadata=metadata)
 
 
 def load_file(filename: Union[str, os.PathLike], device: Union[str, int] = "cpu") -> Dict[str, torch.Tensor]:
@@ -309,6 +317,7 @@ def load_file(filename: Union[str, os.PathLike], device: Union[str, int] = "cpu"
     loaded = load_file(file_path)
     ```
     """
+    return torch.serialization.safe_load_file(filename, device=device)
     result = {}
     with safe_open(filename, framework="pt", device=device) as f:
         for k in f.keys():
@@ -403,7 +412,8 @@ def _view2torch(safeview) -> Dict[str, torch.Tensor]:
 
 
 def _tobytes(tensor: torch.Tensor, name: str) -> bytes:
-    if tensor.layout != torch.strided:
+    if tensor.is_sparse:
+    #if tensor.layout != torch.strided:
         raise ValueError(
             f"You are trying to save a sparse tensor: `{name}` which this library does not support."
             " You can make it a dense tensor before saving with `.to_dense()` but be aware this might"
@@ -417,9 +427,11 @@ def _tobytes(tensor: torch.Tensor, name: str) -> bytes:
             " only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to"
             " pack it before saving."
         )
+
     if tensor.device.type != "cpu":
         # Moving tensor to cpu before saving
-        tensor = tensor.to("cpu")
+        #tensor = tensor.to("cpu")
+        tensor = tensor.cpu()
 
     import ctypes
 
@@ -469,7 +481,8 @@ def _flatten(tensors: Dict[str, torch.Tensor]) -> Dict[str, Dict[str, Any]]:
         if not isinstance(v, torch.Tensor):
             raise ValueError(f"Key `{k}` is invalid, expected torch.Tensor but received {type(v)}")
 
-        if v.layout != torch.strided:
+        if v.is_sparse:
+        #if v.layout != torch.strided:
             invalid_tensors.append(k)
     if invalid_tensors:
         raise ValueError(
