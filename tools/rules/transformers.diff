diff --git a/src/transformers/cache_utils.py b/src/transformers/cache_utils.py
index 23f2177b2..0bcfc28f2 100644
--- a/src/transformers/cache_utils.py
+++ b/src/transformers/cache_utils.py
@@ -242,7 +242,7 @@ class QuantizedCacheConfig(CacheConfig):
         axis_value: Optional[int] = 0,
         q_group_size: Optional[int] = 64,
         residual_length: Optional[int] = 128,
-        compute_dtype: Optional[torch.dtype] = torch.float16,
+        compute_dtype: Optional[int] = torch.float16,
         device: Optional[str] = "cpu",
     ):
         self.backend = backend
@@ -1936,7 +1936,7 @@ class OffloadedStaticCache(StaticCache):
         max_batch_size: int,
         max_cache_len: Optional[int],
         device: Union[str, torch.device],
-        dtype: Optional[torch.dtype] = None,
+        dtype: Optional[int] = None,
         offload_device: Union[str, torch.device] = torch.device("cpu"),
         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,
     ) -> None:
diff --git a/src/transformers/configuration_utils.py b/src/transformers/configuration_utils.py
index e49eab86b..4380f1099 100755
--- a/src/transformers/configuration_utils.py
+++ b/src/transformers/configuration_utils.py
@@ -988,8 +988,10 @@ class PretrainedConfig(PushToHubMixin):
         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *"float32"*
         string, which can then be stored in the json format.
         """
+        # if d.get("torch_dtype", None) is not None and not isinstance(d["torch_dtype"], str):
+        #     d["torch_dtype"] = str(d["torch_dtype"]).split(".")[1]
         if d.get("torch_dtype", None) is not None and not isinstance(d["torch_dtype"], str):
-            d["torch_dtype"] = str(d["torch_dtype"]).split(".")[1]
+            d["torch_dtype"] = str(d["torch_dtype"])
         for value in d.values():
             if isinstance(value, dict):
                 self.dict_torch_dtype_to_str(value)
diff --git a/src/transformers/modeling_utils.py b/src/transformers/modeling_utils.py
index dae29111c..39e62282a 100755
--- a/src/transformers/modeling_utils.py
+++ b/src/transformers/modeling_utils.py
@@ -38,14 +38,14 @@ from huggingface_hub import split_torch_state_dict_into_shards
 from packaging import version
 from torch import Tensor, nn
 from torch.nn import CrossEntropyLoss, Identity
-from torch.utils.checkpoint import checkpoint
+# from torch.utils.checkpoint import checkpoint
 
 from .activations import get_activation
 from .configuration_utils import PretrainedConfig
 from .dynamic_module_utils import custom_object_save
 from .generation import CompileConfig, GenerationConfig, GenerationMixin
 from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled
-from .loss.loss_utils import LOSS_MAPPING
+# from .loss.loss_utils import LOSS_MAPPING
 from .pytorch_utils import (  # noqa: F401
     Conv1D,
     apply_chunking_to_forward,
@@ -380,26 +380,26 @@ def check_support_param_buffer_assignment(model_to_load, state_dict, start_prefi

     Note: We fully disable this if we are using `deepspeed`
     """
-    if model_to_load.device.type == "meta":
-        return False
+    # if model_to_load.device.type == "meta":
+    #     return False

-    if len([key for key in state_dict if key.startswith(start_prefix)]) == 0:
-        return False
+    # if len([key for key in state_dict if key.startswith(start_prefix)]) == 0:
+    #     return False

-    if is_deepspeed_zero3_enabled():
-        return False
+    # if is_deepspeed_zero3_enabled():
+    #     return False

-    # Some models explicitly do not support param buffer assignment
-    if not getattr(model_to_load, "_supports_param_buffer_assignment", True):
-        logger.debug(
-            f"{model_to_load.__class__.__name__} does not support param buffer assignment, loading will be slower"
-        )
-        return False
+    # # Some models explicitly do not support param buffer assignment
+    # if not getattr(model_to_load, "_supports_param_buffer_assignment", True):
+    #     logger.debug(
+    #         f"{model_to_load.__class__.__name__} does not support param buffer assignment, loading will be slower"
+    #     )
+    #     return False

-    # If the model does, the incoming `state_dict` and the `model_to_load` must be the same dtype
-    first_key = next(iter(model_to_load.state_dict().keys()))
-    if start_prefix + first_key in state_dict:
-        return state_dict[start_prefix + first_key].dtype == model_to_load.state_dict()[first_key].dtype
+    # # If the model does, the incoming `state_dict` and the `model_to_load` must be the same dtype
+    # first_key = next(iter(model_to_load.state_dict().keys()))
+    # if start_prefix + first_key in state_dict:
+    #     return state_dict[start_prefix + first_key].dtype == model_to_load.state_dict()[first_key].dtype

     # For cases when the `state_dict` doesn't contain real weights to the model (`test_model_weights_reload_no_missing_tied_weights`)
     return False
@@ -501,13 +501,15 @@ def load_state_dict(
     """
     if checkpoint_file.endswith(".safetensors") and is_safetensors_available():
         # Check format of the archive
-        with safe_open(checkpoint_file, framework="pt") as f:
-            metadata = f.metadata()
-        if metadata.get("format") not in ["pt", "tf", "flax", "mlx"]:
-            raise OSError(
-                f"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure "
-                "you save your model with the `save_pretrained` method."
-            )
+
+        # with safe_open(checkpoint_file, framework="pt") as f:
+        #     metadata = f.metadata()
+        # if metadata.get("format") not in ["pt", "tf", "flax", "mlx"]:
+        #     raise OSError(
+        #         f"The safetensors archive passed at {checkpoint_file} does not contain the valid metadata. Make sure "
+        #         "you save your model with the `save_pretrained` method."
+        #     )
+        from torch.serialization import safe_load_file
         return safe_load_file(checkpoint_file)
     try:
         if map_location is None:
@@ -1519,7 +1521,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         cls,
         config,
         use_flash_attention_2: bool = False,
-        torch_dtype: Optional[torch.dtype] = None,
+        torch_dtype: Optional[int] = None,
         device_map: Optional[Union[str, Dict[str, int]]] = None,
         check_device_map: bool = True,
     ):
@@ -1599,15 +1601,15 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                 hard_check_only=False if requested_attn_implementation is None else True,
             )

-            if (
-                torch.version.hip is not None
-                and config._attn_implementation == "sdpa"
-                and torch.cuda.device_count() > 1
-            ):
-                logger.warning_once(
-                    "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends."
-                )
-                torch.backends.cuda.enable_flash_sdp(False)
+            # if (
+            #     torch.version.hip is not None
+            #     and config._attn_implementation == "sdpa"
+            #     and torch.cuda.device_count() > 1
+            # ):
+            #     logger.warning_once(
+            #         "Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends."
+            #     )
+            #     torch.backends.cuda.enable_flash_sdp(False)
         elif isinstance(requested_attn_implementation, dict):
             config._attn_implementation = None
         else:
@@ -1633,10 +1635,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         Note `set_default_dtype` currently only works with floating-point types and asserts if for example,
         `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.
         """
-        if not dtype.is_floating_point:
-            raise ValueError(
-                f"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype"
-            )
+        # if not dtype.is_floating_point:
+        #     raise ValueError(
+        #         f"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype"
+        #     )

         logger.info(f"Instantiating {cls.__name__} model under default dtype {dtype}.")
         dtype_orig = torch.get_default_dtype()
@@ -1693,7 +1695,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
     def _check_and_enable_flash_attn_2(
         cls,
         config,
-        torch_dtype: Optional[torch.dtype] = None,
+        torch_dtype: Optional[int] = None,
         device_map: Optional[Union[str, Dict[str, int]]] = None,
         check_device_map: bool = True,
         hard_check_only: bool = False,
@@ -2619,7 +2621,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
             # the gradients to make sure the gradient flows.
             self.enable_input_require_grads()

-    def _set_gradient_checkpointing(self, enable: bool = True, gradient_checkpointing_func: Callable = checkpoint):
+    def _set_gradient_checkpointing(self, enable, gradient_checkpointing_func): # Callable = checkpoint):
         is_gradient_checkpointing_set = False

         # Apply it on the top-level module in case the top-level modules supports it
@@ -4487,7 +4489,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
         if device_map is None and not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():
             ptrs = collections.defaultdict(list)
             for name, tensor in model.state_dict().items():
-                id_tensor = id_tensor_storage(tensor)
+                # id_tensor = id_tensor_storage(tensor)
+                id_tensor = id(tensor)
                 ptrs[id_tensor].append(name)

             # These are all the pointers of shared tensors.
@@ -4554,6 +4557,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix
                         hf_quantizer.create_quantized_param(model, value, key, "cpu", state_dict, unexpected_keys)

         # retrieve uninitialized modules and initialize before maybe overriding that with the pretrained weights.
+        _fast_init = False
         if _fast_init:
             if not ignore_mismatched_sizes:
                 if remove_prefix_from_model:
diff --git a/src/transformers/models/t5/modeling_t5.py b/src/transformers/models/t5/modeling_t5.py
index 9012c8db9..d53d398ea 100644
--- a/src/transformers/models/t5/modeling_t5.py
+++ b/src/transformers/models/t5/modeling_t5.py
@@ -245,14 +245,12 @@ class T5LayerNorm(nn.Module):
         # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated
         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for
         # half-precision inputs is done in fp32
-
         variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

         # convert into half-precision if necessary
         if self.weight.dtype in [torch.float16, torch.bfloat16]:
             hidden_states = hidden_states.to(self.weight.dtype)
-
         return self.weight * hidden_states


@@ -434,9 +432,13 @@ class T5Attention(nn.Module):
             / math.log(max_distance / max_exact)
             * (num_buckets - max_exact)
         ).to(torch.long)
-        relative_position_if_large = torch.min(
-            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)
-        )
+        relative_position_buckets = torch.full_like(relative_position_if_large, num_buckets - 1)
+        mask = (relative_position_if_large - relative_position_buckets) < 0
+        mask = mask.astype(relative_position_if_large.dtype)
+        relative_position_if_large = relative_position_if_large * mask + relative_position_buckets * (1 - mask)
+        # relative_position_if_large = torch.min(
+        #     relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)
+        # )

         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)
         return relative_buckets
@@ -448,8 +450,8 @@ class T5Attention(nn.Module):
         if cache_position is None:
             context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
         else:
-            context_position = cache_position[:, None].to(device)
-        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
+            context_position = cache_position[:, None]
+        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]
         relative_position = memory_position - context_position  # shape (query_length, key_length)
         relative_position_bucket = self._relative_position_bucket(
             relative_position,  # shape (query_length, key_length)
@@ -459,6 +461,7 @@ class T5Attention(nn.Module):
         )
         values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)
         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)
+
         return values

     def forward(
@@ -684,7 +687,6 @@ class T5Block(nn.Module):
         )
         hidden_states, past_key_value = self_attention_outputs[:2]
         attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights
-
         # clamp inf values to enable fp16 training
         if hidden_states.dtype == torch.float16:
             clamp_value = torch.where(
@@ -723,7 +725,6 @@ class T5Block(nn.Module):

         # Apply Feed Forward layer
         hidden_states = self.layer[-1](hidden_states)
-
         # clamp inf values to enable fp16 training
         if hidden_states.dtype == torch.float16:
             clamp_value = torch.where(
@@ -881,7 +882,7 @@ class T5PreTrainedModel(PreTrainedModel):
 class T5Stack(T5PreTrainedModel):
     def __init__(self, config, embed_tokens=None):
         super().__init__(config)
-
+        # config.num_layers = 1
         self.embed_tokens = embed_tokens
         self.is_decoder = config.is_decoder

@@ -1051,7 +1052,12 @@ class T5Stack(T5PreTrainedModel):
         elif attention_mask is not None:
             causal_mask = attention_mask[:, None, None, :]
             causal_mask = causal_mask.to(dtype=inputs_embeds.dtype)
-            causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min
+            if inputs_embeds.dtype == torch.bfloat16:
+                finfo = -3.38953e+38
+            else:
+                finfo = torch.finfo(inputs_embeds.dtype).min
+            causal_mask = (1.0 - causal_mask) * finfo
+            # causal_mask = (1.0 - causal_mask) * torch.finfo(inputs_embeds.dtype).min
         else:
             causal_mask = None

@@ -1078,7 +1084,6 @@ class T5Stack(T5PreTrainedModel):
         encoder_decoder_position_bias = None

         hidden_states = self.dropout(inputs_embeds)
-
         for i, layer_module in enumerate(self.block):
             layer_head_mask = head_mask[i]
             cross_attn_layer_head_mask = cross_attn_head_mask[i]
diff --git a/src/transformers/pytorch_utils.py b/src/transformers/pytorch_utils.py
index 5bdf8a355..2e138e986 100644
--- a/src/transformers/pytorch_utils.py
+++ b/src/transformers/pytorch_utils.py
@@ -301,16 +301,16 @@ def id_tensor_storage(tensor: torch.Tensor) -> Tuple[torch.device, int, int]:
     guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with
     non-overlapping lifetimes may have the same id.
     """
-    if tensor.device.type == "xla" and is_torch_xla_available():
-        # NOTE: xla tensors dont have storage
-        # use some other unique id to distinguish.
-        # this is a XLA tensor, it must be created using torch_xla's
-        # device. So the following import is safe:
-        import torch_xla
-
-        unique_id = torch_xla._XLAC._xla_get_tensor_id(tensor)
-    else:
-        unique_id = storage_ptr(tensor)
+    # if tensor.device.type == "xla" and is_torch_xla_available():
+    #     # NOTE: xla tensors dont have storage
+    #     # use some other unique id to distinguish.
+    #     # this is a XLA tensor, it must be created using torch_xla's
+    #     # device. So the following import is safe:
+    #     import torch_xla
+
+    #     unique_id = torch_xla._XLAC._xla_get_tensor_id(tensor)
+    # else:
+    unique_id = storage_ptr(tensor)

     return tensor.device, unique_id, storage_size(tensor)

diff --git a/src/transformers/utils/import_utils.py b/src/transformers/utils/import_utils.py
index 32a647594..d17ae5c02 100755
--- a/src/transformers/utils/import_utils.py
+++ b/src/transformers/utils/import_utils.py
@@ -194,13 +194,14 @@ _blobfile_available = _is_package_available("blobfile")
 _liger_kernel_available = _is_package_available("liger_kernel")


-_torch_version = "N/A"
-_torch_available = False
-if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:
-    _torch_available, _torch_version = _is_package_available("torch", return_version=True)
-else:
-    logger.info("Disabling PyTorch because USE_TF is set")
-    _torch_available = False
+_torch_version = "2.1.0"
+_torch_available = True
+# _torch_available = False
+# if USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:
+#     _torch_available, _torch_version = _is_package_available("torch", return_version=True)
+# else:
+#     logger.info("Disabling PyTorch because USE_TF is set")
+#     _torch_available = False


 _tf_version = "N/A"
