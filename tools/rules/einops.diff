diff -Naur einops/_backends.py einops_new/_backends.py
--- einops/_backends.py 2025-05-21 23:30:44.358542280 +0800
+++ einops_new/_backends.py 2025-05-21 23:31:26.198542280 +0800
@@ -713,3 +713,75 @@
 
     def einsum(self, pattern, *x):
         return self.pt.einsum(pattern, *x)
+
+
+class MindSporeBackend(AbstractBackend):
+    framework_name = "mindspore"
+
+    def __init__(self):
+        import mindspore
+
+        self.mindspore = mindspore
+        # importing would register operations in torch._dyname for torch.compile
+        # from . import _torch_specific  # noqa
+
+    def is_appropriate_type(self, tensor):
+        return isinstance(tensor, self.mindspore.Tensor)
+
+    def from_numpy(self, x):
+        variable = self.mindspore.from_numpy(x)
+        if self.is_float_type(variable):
+            # attach grad only to floating types
+            variable.requires_grad = True
+        return variable
+
+    def to_numpy(self, x):
+        return x.detach().cpu().numpy()
+
+    def arange(self, start, stop):
+        return self.mindspore.arange(start, stop, dtype=self.mindspore.int64)
+
+    def reduce(self, x, operation, reduced_axes):
+        if operation == "min":
+            return x.amin(dim=reduced_axes)
+        elif operation == "max":
+            return x.amax(dim=reduced_axes)
+        elif operation == "sum":
+            return x.sum(dim=reduced_axes)
+        elif operation == "mean":
+            return x.mean(dim=reduced_axes)
+        elif operation in ("any", "all", "prod"):
+            # mindspore supports reducing only one operation at a time
+            for i in list(sorted(reduced_axes))[::-1]:
+                x = getattr(x, operation)(dim=i)
+            return x
+        else:
+            raise NotImplementedError("Unknown reduction ", operation)
+
+    def transpose(self, x, axes):
+        return x.permute(axes)
+
+    def stack_on_zeroth_dimension(self, tensors: list):
+        return self.mindspore.stack(tensors)
+
+    def add_axes(self, x, n_axes, pos2len):
+        repeats = [-1] * n_axes
+        for axis_position, axis_length in pos2len.items():
+            x = self.add_axis(x, axis_position)
+            repeats[axis_position] = axis_length
+        return x.expand(repeats)
+
+    def tile(self, x, repeats):
+        return x.repeat(repeats)
+
+    def concat(self, tensors, axis: int):
+        return self.mindspore.cat(tensors, dim=axis)
+
+    def add_axis(self, x, new_position):
+        return self.mindspore.unsqueeze(x, new_position)
+
+    def is_float_type(self, x):
+        return x.dtype in [self.mindspore.float16, self.mindspore.float32, self.mindspore.float64, self.mindspore.bfloat16]
+
+    def einsum(self, pattern, *x):
+        return self.mindspore.einsum(pattern, *x)
