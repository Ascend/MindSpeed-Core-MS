diff --git a/examples/qwen2vl/dot_product_attention.py b/examples/qwen2vl/dot_product_attention.py
index b6e9d33..8718216 100644
--- a/examples/qwen2vl/dot_product_attention.py
+++ b/examples/qwen2vl/dot_product_attention.py
@@ -14,8 +14,8 @@ from torch import Tensor
 from megatron.training import get_args
 from megatron.core import mpu
 from mindspeed.core.models.common.embeddings.rotary_pos_embedding import yarn_get_mscale
-from mindspeed.core.context_parallel.ring_context_parallel import ringattn_context_parallel
-from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention
+#from mindspeed.core.context_parallel.ring_context_parallel import ringattn_context_parallel
+#from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention
 from mindspeed.model.alibi_mask import AlibiForFusionAttnSingleton
 from mindspeed.core.parallel_state import (get_context_parallel_group_for_hybrid_ring,
                                            get_context_parallel_for_hybrid_ring_world_size,
@@ -115,7 +115,7 @@ def dot_product_attention_forward(
             padding_mask=None,
             atten_mask=None,
             scale=1.0 / math.sqrt(query.shape[-1]),
-            keep_prob=1,
+            keep_prob=1.0,
             input_layout='TND',
             actual_seq_qlen=actual_seq_len,
             actual_seq_kvlen=actual_seq_len,
@@ -137,7 +137,7 @@ def dot_product_attention_forward(
             padding_mask=None,
             atten_mask=attention_mask_npu,
             scale=1.0 / math.sqrt(query.shape[-1]),
-            keep_prob=1,
+            keep_prob=1.0,
             input_layout='TND',
             actual_seq_qlen=tuple(cu_seq_lens[1:].cpu().numpy().tolist()),
             actual_seq_kvlen=tuple(cu_seq_lens[1:].cpu().numpy().tolist()),
@@ -198,9 +198,10 @@ class IndexFirstAxis(torch.autograd.Function):
     def forward(ctx, input_tensor, indices):
         ctx.save_for_backward(indices)
         ctx.first_axis_dim, other_shape = input_tensor.shape[0], input_tensor.shape[1:]
+        other_shape = torch.Size(other_shape)
         second_dim = other_shape.numel()
         return torch.gather(
-            rearrange(input_tensor, "b ... -> b (...)"), 0, repeat(indices, "z -> z d", d=second_dim)
+            rearrange(input_tensor, "b ... -> b (...)").contiguous(), 0, repeat(indices, "z -> z d", d=second_dim).contiguous()
         ).reshape(-1, *other_shape)
 
     @staticmethod
diff --git a/examples/qwen2vl/qwen2vl_convert_to_mm_ckpt.py b/examples/qwen2vl/qwen2vl_convert_to_mm_ckpt.py
index 4daf73d..676aff6 100644
--- a/examples/qwen2vl/qwen2vl_convert_to_mm_ckpt.py
+++ b/examples/qwen2vl/qwen2vl_convert_to_mm_ckpt.py
@@ -4,7 +4,7 @@ from pathlib import Path
 from copy import deepcopy
 
 import torch
-from safetensors.torch import load_file
+from torch.serialization import safe_load_file
 
 
 def load_from_hf(_load_dir):
@@ -12,7 +12,7 @@ def load_from_hf(_load_dir):
     load_dir = Path(_load_dir)
     state_dict = {}
     for safe_path in load_dir.glob("*.safetensors"):
-        state_dict.update(load_file(str(safe_path), device='cpu'))
+        state_dict.update(safe_load_file(str(safe_path), device='cpu'))
     return state_dict
 
 
diff --git a/mindspeed_mm/__init__.py b/mindspeed_mm/__init__.py
index 24ee0b3..f4fce2b 100644
--- a/mindspeed_mm/__init__.py
+++ b/mindspeed_mm/__init__.py
@@ -14,7 +14,7 @@ from mindspeed_mm.models import (
     VLModel
 )
 from mindspeed_mm.patchs import PatchesManager
-from mindspeed_mm.tasks import sora_pipeline_dict, vlm_pipeline_dict
+# from mindspeed_mm.tasks import sora_pipeline_dict, vlm_pipeline_dict
 from mindspeed_mm.utils.utils import (
     is_npu_available,
     get_device,
diff --git a/mindspeed_mm/data/data_utils/func_utils/mm_plugin.py b/mindspeed_mm/data/data_utils/func_utils/mm_plugin.py
index dc2ebb1..321f0e8 100644
--- a/mindspeed_mm/data/data_utils/func_utils/mm_plugin.py
+++ b/mindspeed_mm/data/data_utils/func_utils/mm_plugin.py
@@ -282,7 +282,7 @@ class Qwen2vlPlugin(BasePlugin):
                 content = content.replace(
                     IMAGE_PLACEHOLDER,
                     "<|vision_start|>{}<|vision_end|>".format(
-                        self.image_token * (image_grid_thw[num_image_tokens].prod() // merge_length)
+                        self.image_token * int(image_grid_thw[num_image_tokens].prod() // merge_length)
                     ),
                     1,
                 )
diff --git a/mindspeed_mm/data/data_utils/utils.py b/mindspeed_mm/data/data_utils/utils.py
index 577cba1..272b1a2 100644
--- a/mindspeed_mm/data/data_utils/utils.py
+++ b/mindspeed_mm/data/data_utils/utils.py
@@ -40,7 +40,6 @@ from torchvision.transforms import InterpolationMode
 from torchvision.io.video import (
     _align_audio_frames,
     _check_av_available,
-    _log_api_usage_once,
     _read_from_stream,
     _video_opt,
 )
diff --git a/mindspeed_mm/data/datasets/qwen2vl_dataset.py b/mindspeed_mm/data/datasets/qwen2vl_dataset.py
index 396cbf2..faf170d 100644
--- a/mindspeed_mm/data/datasets/qwen2vl_dataset.py
+++ b/mindspeed_mm/data/datasets/qwen2vl_dataset.py
@@ -1,71 +1,78 @@
-import os
 from functools import partial
 
 from datasets import load_dataset
-from torch.utils.data import Dataset
-from transformers.training_args import TrainingArguments
+from torch.utils.data import Dataset, ConcatDataset
+from transformers import AutoProcessor
 
 from mindspeed_mm.data.data_utils.func_utils.convert import DataArguments, DatasetAttr, load_tokenizer, \
     convert_sharegpt, preprocess_supervised_dataset
-from mindspeed_mm.data.data_utils.func_utils.log import get_logger
 from mindspeed_mm.data.data_utils.func_utils.model_args import ProcessorArguments
 from mindspeed_mm.data.data_utils.func_utils.template import get_template_and_fix_tokenizer
 
-logger = get_logger(__name__)
-
 
 def get_qwen2vl_dataset(basic_param, preprocess_param, dataset_param):
     data_args = DataArguments(**basic_param)
     process_args = ProcessorArguments(**preprocess_param)
     dataset_attr = DatasetAttr(**dataset_param["attr"])
-
     tokenizer_module = load_tokenizer(process_args)
-    tokenizer, processor = tokenizer_module['tokenizer'], tokenizer_module['processor']
+    tokenizer = tokenizer_module['tokenizer']
+    processor = AutoProcessor.from_pretrained(process_args.model_name_or_path, local_files_only=True)
     template = get_template_and_fix_tokenizer(tokenizer, data_args.template)
-    # 确保主进程进行数据处理，其他进程复用缓存避免重复计算，该策略和llamafactory对数据处理策略一致
-    with TrainingArguments(output_dir='./').main_process_first(desc="pre-process dataset"):
-        # -----------------load dataset from file-------------------------------------------------------------------------
-        dataset = load_dataset(path="json", data_files=data_args.dataset, split="train", cache_dir=data_args.cache_dir,
-                               streaming=data_args.streaming)
-        if data_args.max_samples:
-            dataset = dataset.select(range(data_args.max_samples))
-        local_process_index = int(os.getenv("LOCAL_RANK", -1))
-        if data_args.streaming:
-            kwargs = {}
-        else:
-            kwargs = {
-                "num_proc": data_args.preprocessing_num_workers,
-                # 配置了overwrite_cache为false（默认为false)时，非rank0节点读取cache不再进行map处理
-                # 配置了overwrite_cache为true（默认为false)时，所有节点都读取cache不再进行map处理
-                "load_from_cache_file": (not data_args.overwrite_cache) or (local_process_index != 0)
-            }
-        logger.debug(f'Rank: %s, kwargs: %s', local_process_index, kwargs)
-        # -----------------convert to sharegpt ---------------------------------------------------------------------------
-        convert_func = partial(convert_sharegpt, dataset_attr=dataset_attr, dataset_dir=data_args.dataset_dir)
-        dataset = dataset.map(
-            convert_func,
-            batched=False,
-            remove_columns=(list(next(iter(dataset)).keys())),
-            desc=f"Rank {local_process_index}, Converting format of dataset",
-            **kwargs,
-        )
-        # -----------------convert text to token id ----------------------------------------------------------------------
-        preprocess_func = partial(
-            preprocess_supervised_dataset,
-            template=template,
-            tokenizer=tokenizer,
-            processor=processor,
-            data_args=data_args,
+    # -----------------load dataset from file-------------------------------------------------------------------------
+    dataset = load_dataset(
+        path="json",
+        name=None,
+        data_dir=None,
+        data_files=data_args.dataset,
+        split="train",
+        cache_dir=data_args.cache_dir,
+        token=None,
+        streaming=data_args.streaming,
+        trust_remote_code=False,
+    )
+    # -----------------convert to sharegpt ---------------------------------------------------------------------------
+    convert_func = partial(convert_sharegpt, dataset_attr=dataset_attr, dataset_dir=data_args.dataset_dir)
+    column_names = list(next(iter(dataset)).keys())
+    kwargs = {}
+    if not data_args.streaming:
+        local_process_index = 0
+        kwargs = dict(
+            num_proc=data_args.preprocessing_num_workers,
+            load_from_cache_file=(not data_args.overwrite_cache) or (local_process_index != 0),
+            desc="Converting format of dataset",
         )
-        dataset = dataset.map(
-            preprocess_func,
-            batched=True,
-            batch_size=data_args.preprocessing_batch_size,
-            remove_columns=(list(next(iter(dataset)).keys())),
-            desc=f"Rank {local_process_index}, Running tokenizer on dataset",
-            **kwargs,
+    if data_args.max_samples:
+        dataset = dataset.select(range(data_args.max_samples))
+    dataset = dataset.map(
+        convert_func,
+        batched=False,
+        remove_columns=column_names,
+        **kwargs,
+    )
+    # -----------------convert text to token id ----------------------------------------------------------------------
+    preprocess_func = partial(
+        preprocess_supervised_dataset,
+        template=template,
+        tokenizer=tokenizer,
+        processor=processor,
+        data_args=data_args,
+    )
+    column_names = list(next(iter(dataset)).keys())
+    kwargs = {}
+    if not data_args.streaming:
+        kwargs = dict(
+            num_proc=data_args.preprocessing_num_workers,
+            load_from_cache_file=(not data_args.overwrite_cache) or (local_process_index != 0),
+            desc="Running tokenizer on dataset",
         )
-        return dataset
+    dataset = dataset.map(
+        preprocess_func,
+        batched=True,
+        batch_size=data_args.preprocessing_batch_size,
+        remove_columns=column_names,
+        **kwargs,
+    )
+    return dataset
 
 
 class Qwen2vlDataset(Dataset):
diff --git a/mindspeed_mm/models/ae/vae.py b/mindspeed_mm/models/ae/vae.py
index 53a7d6d..d6d85fe 100644
--- a/mindspeed_mm/models/ae/vae.py
+++ b/mindspeed_mm/models/ae/vae.py
@@ -1,6 +1,6 @@
 import torch
 import torch.nn as nn
-from diffusers.models import AutoencoderKL
+# from diffusers.models import AutoencoderKL
 from einops import rearrange
 from megatron.core import mpu
 
diff --git a/mindspeed_mm/models/common/blocks.py b/mindspeed_mm/models/common/blocks.py
index a4eeff0..1458385 100644
--- a/mindspeed_mm/models/common/blocks.py
+++ b/mindspeed_mm/models/common/blocks.py
@@ -16,7 +16,7 @@ from typing import Optional
 import torch
 import torch.nn as nn
 
-from diffusers.models.activations import GELU, GEGLU, ApproximateGELU
+# from diffusers.models.activations import GELU, GEGLU, ApproximateGELU
 from mindspeed_mm.models.common.linear import MatmulAddLinear
 
 
diff --git a/mindspeed_mm/models/common/checkpoint.py b/mindspeed_mm/models/common/checkpoint.py
index 8d0b05a..c3b6a03 100644
--- a/mindspeed_mm/models/common/checkpoint.py
+++ b/mindspeed_mm/models/common/checkpoint.py
@@ -3,7 +3,7 @@ import os
 
 import torch
 import torch.nn as nn
-from torch.utils.checkpoint import checkpoint, checkpoint_sequential
+# from torch.utils.checkpoint import checkpoint, checkpoint_sequential
 
 import safetensors
 
diff --git a/mindspeed_mm/models/common/embeddings/common_embeddings.py b/mindspeed_mm/models/common/embeddings/common_embeddings.py
index 4c6b6d7..290d523 100644
--- a/mindspeed_mm/models/common/embeddings/common_embeddings.py
+++ b/mindspeed_mm/models/common/embeddings/common_embeddings.py
@@ -3,7 +3,7 @@ from einops import rearrange
 
 import torch
 from torch import nn
-from timm.models.vision_transformer import Mlp
+# from timm.models.vision_transformer import Mlp
 
 
 class TimestepEmbedder(nn.Module):
diff --git a/mindspeed_mm/models/common/embeddings/pos_embeddings.py b/mindspeed_mm/models/common/embeddings/pos_embeddings.py
index 420d2e6..8e36461 100644
--- a/mindspeed_mm/models/common/embeddings/pos_embeddings.py
+++ b/mindspeed_mm/models/common/embeddings/pos_embeddings.py
@@ -533,8 +533,8 @@ class Rotary3DPositionEmbedding(nn.Module):
         freqs = broad_cat((freqs_t[:, None, None, :], freqs_h[None, :, None, :], freqs_w[None, None, :, :]), dim=-1)
 
         freqs = freqs.contiguous()
-        self.freqs_sin = freqs.sin().npu()
-        self.freqs_cos = freqs.cos().npu()
+        self.freqs_sin = freqs.sin()
+        self.freqs_cos = freqs.cos()
 
         self.text_length = text_length
         if learnable_pos_embed:
diff --git a/mindspeed_mm/models/common/embeddings/time_embeddings.py b/mindspeed_mm/models/common/embeddings/time_embeddings.py
index 6bb7f33..4174769 100644
--- a/mindspeed_mm/models/common/embeddings/time_embeddings.py
+++ b/mindspeed_mm/models/common/embeddings/time_embeddings.py
@@ -1,5 +1,5 @@
 import math
-
+import numpy as np
 import torch
 import torch.nn as nn
 from einops import rearrange, repeat
@@ -16,9 +16,9 @@ def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False, dtyp
     """
     if not repeat_only:
         half = dim // 2
-        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(
-            device=timesteps.device
-        )
+
+        tmp = -math.log(max_period) * np.arange(0, half) / half
+        freqs = torch.exp(torch.Tensor(tmp.astype(np.float32)))
         args = timesteps[:, None].float() * freqs[None]
         embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
         if dim % 2:
diff --git a/mindspeed_mm/models/common/ffn.py b/mindspeed_mm/models/common/ffn.py
index 89bcdca..1e82d26 100644
--- a/mindspeed_mm/models/common/ffn.py
+++ b/mindspeed_mm/models/common/ffn.py
@@ -3,7 +3,7 @@ from typing import Optional
 import torch
 from torch import nn
 import torch.nn.functional as F
-from diffusers.models.activations import GEGLU, ApproximateGELU
+# from diffusers.models.activations import GEGLU, ApproximateGELU
 from megatron.core import mpu, tensor_parallel
 from megatron.training import get_args
 from megatron.training.arguments import core_transformer_config_from_args
diff --git a/mindspeed_mm/models/common/regularizer.py b/mindspeed_mm/models/common/regularizer.py
index 2dc215c..938035a 100644
--- a/mindspeed_mm/models/common/regularizer.py
+++ b/mindspeed_mm/models/common/regularizer.py
@@ -15,10 +15,10 @@ class DiagonalGaussianDistribution(object):
         self.std = torch.exp(0.5 * self.logvar)
         self.var = torch.exp(self.logvar)
         if self.deterministic:
-            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)
+            self.var = self.std = torch.zeros_like(self.mean)
 
     def sample(self):
-        x = self.mean + self.std * torch.randn_like(self.mean).to(device=self.parameters.device)
+        x = self.mean + self.std * torch.randn_like(self.mean)
         return x
 
     def kl(self, other=None):
diff --git a/mindspeed_mm/models/diffusion/cogvideo_diffusion.py b/mindspeed_mm/models/diffusion/cogvideo_diffusion.py
index 85dbf37..53ed119 100644
--- a/mindspeed_mm/models/diffusion/cogvideo_diffusion.py
+++ b/mindspeed_mm/models/diffusion/cogvideo_diffusion.py
@@ -95,10 +95,10 @@ def make_beta_schedule(
     linear_end=2e-2,
 ):
     if schedule == "linear":
-        betas = torch.linspace(linear_start**0.5, linear_end**0.5, n_timestep, dtype=torch.float64) ** 2
+        betas = np.linspace(linear_start**0.5, linear_end**0.5, n_timestep, dtype=np.float64) ** 2
     else:
         raise NotImplementedError("Only support linear schedule")
-    return betas.numpy()
+    return betas
 
 
 class Discretization:
@@ -220,11 +220,11 @@ class DiscreteDenoiser(nn.Module):
         return c_in, c_noise, c_out, c_skip
 
     def sigma_to_idx(self, sigma):
-        dists = sigma - self.sigmas.to(sigma.device)[:, None]
+        dists = sigma - self.sigmas[:, None]
         return dists.abs().argmin(dim=0).view(sigma.shape)
 
     def idx_to_sigma(self, idx):
-        return self.sigmas.to(idx.device)[idx]
+        return self.sigmas[idx]
 
     def possibly_quantize_sigma(self, sigma):
         return self.idx_to_sigma(self.sigma_to_idx(sigma))
@@ -285,8 +285,8 @@ class CogVideoDiffusion(nn.Module):
 
         additional_model_inputs = dict()
         alphas_cumprod_sqrt, idx = self.sigma_sampler(latents.shape[0], return_idx=True)
-        self.alphas_cumprod_sqrt = alphas_cumprod_sqrt.to(latents.device)
-        idx = idx.to(latents.device)
+        self.alphas_cumprod_sqrt = alphas_cumprod_sqrt
+        idx = idx
 
         # broadcast noise here
 
@@ -294,7 +294,7 @@ class CogVideoDiffusion(nn.Module):
 
         if self.offset_noise_level > 0.0:
             noise = (
-                    noise + append_dims(torch.randn(latents.shape[0]).to(latents.device),
+                    noise + append_dims(torch.randn(latents.shape[0]),
                                         latents.ndim) * self.offset_noise_level
             )
 
diff --git a/mindspeed_mm/models/diffusion/diffusers_scheduler.py b/mindspeed_mm/models/diffusion/diffusers_scheduler.py
index a3ab791..e41d882 100644
--- a/mindspeed_mm/models/diffusion/diffusers_scheduler.py
+++ b/mindspeed_mm/models/diffusion/diffusers_scheduler.py
@@ -7,38 +7,38 @@ import torch.distributed as dist
 from torch import Tensor
 from tqdm.auto import tqdm
 import torch.nn.functional as F
-from diffusers.schedulers import (
-    DDIMScheduler,
-    DDPMScheduler,
-    PNDMScheduler,
-    EulerDiscreteScheduler,
-    DPMSolverMultistepScheduler,
-    HeunDiscreteScheduler,
-    EulerAncestralDiscreteScheduler,
-    DEISMultistepScheduler,
-    KDPM2AncestralDiscreteScheduler,
-    CogVideoXDPMScheduler,
-    CogVideoXDDIMScheduler,
-    FlowMatchEulerDiscreteScheduler
-)
-from diffusers.training_utils import compute_snr
+# from diffusers.schedulers import (
+#     DDIMScheduler,
+#     DDPMScheduler,
+#     PNDMScheduler,
+#     EulerDiscreteScheduler,
+#     DPMSolverMultistepScheduler,
+#     HeunDiscreteScheduler,
+#     EulerAncestralDiscreteScheduler,
+#     DEISMultistepScheduler,
+#     KDPM2AncestralDiscreteScheduler,
+#     CogVideoXDPMScheduler,
+#     CogVideoXDDIMScheduler,
+#     FlowMatchEulerDiscreteScheduler
+# )
+# from diffusers.training_utils import compute_snr
 from megatron.core import mpu
 
 from mindspeed_mm.models.diffusion.diffusion_utils import explicit_uniform_sampling
 from mindspeed_mm.utils.utils import get_device
 
 DIFFUSERS_SCHEDULE_MAPPINGS = {
-    "DDIM": DDIMScheduler,
-    "EulerDiscrete": EulerDiscreteScheduler,
-    "DDPM": DDPMScheduler,
-    "DPMSolverMultistep": DPMSolverMultistepScheduler,
-    "PNDM": PNDMScheduler,
-    "HeunDiscrete": HeunDiscreteScheduler,
-    "EulerAncestralDiscrete": EulerAncestralDiscreteScheduler,
-    "DEISMultistep": DEISMultistepScheduler,
-    "KDPM2AncestralDiscrete": KDPM2AncestralDiscreteScheduler,
-    "cogvideox_5b": CogVideoXDPMScheduler,
-    "cogvideox_2b": CogVideoXDDIMScheduler
+    # "DDIM": DDIMScheduler,
+    # "EulerDiscrete": EulerDiscreteScheduler,
+    # "DDPM": DDPMScheduler,
+    # "DPMSolverMultistep": DPMSolverMultistepScheduler,
+    # "PNDM": PNDMScheduler,
+    # "HeunDiscrete": HeunDiscreteScheduler,
+    # "EulerAncestralDiscrete": EulerAncestralDiscreteScheduler,
+    # "DEISMultistep": DEISMultistepScheduler,
+    # "KDPM2AncestralDiscrete": KDPM2AncestralDiscreteScheduler,
+    # "cogvideox_5b": CogVideoXDPMScheduler,
+    # "cogvideox_2b": CogVideoXDDIMScheduler
 }
 
 
diff --git a/mindspeed_mm/models/diffusion/rflow.py b/mindspeed_mm/models/diffusion/rflow.py
index c0278f3..52011c1 100644
--- a/mindspeed_mm/models/diffusion/rflow.py
+++ b/mindspeed_mm/models/diffusion/rflow.py
@@ -1,7 +1,7 @@
 from tqdm.auto import tqdm
 import torch
 from torch import Tensor
-from torch.distributions import LogisticNormal
+# from torch.distributions import LogisticNormal
 
 from .diffusion_utils import extract_into_tensor, mean_flat
 
diff --git a/mindspeed_mm/models/internvl_model.py b/mindspeed_mm/models/internvl_model.py
index bd5c9ca..39b3ab1 100644
--- a/mindspeed_mm/models/internvl_model.py
+++ b/mindspeed_mm/models/internvl_model.py
@@ -258,7 +258,8 @@ class InternVLModel(MultiModalModule):
             Make causal mask used for bi-directional self-attention.
             """
             bsz, tgt_len = input_ids_shape
-            mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)
+            type_min = -3.4028234663852886e+38
+            mask = torch.full((tgt_len, tgt_len), torch.tensor(type_min))
             mask_cond = torch.arange(mask.size(-1), device=device)
             mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
             mask = mask.to(dtype)
@@ -278,8 +279,8 @@ class InternVLModel(MultiModalModule):
             expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
 
             inverted_mask = 1.0 - expanded_mask
-
-            return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)
+            type_min = -3.4028234663852886e+38
+            return inverted_mask.masked_fill(inverted_mask.to(torch.bool), type_min)
 
         input_shape = attention_mask.shape
         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
@@ -294,7 +295,7 @@ class InternVLModel(MultiModalModule):
 
         if attention_mask is not None:
             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
-            expanded_attn_mask = _expand_mask(attention_mask, dtype, tgt_len=input_shape[-1]).to(device)
+            expanded_attn_mask = _expand_mask(attention_mask, dtype, tgt_len=input_shape[-1])
             combined_attention_mask = (
                 expanded_attn_mask if combined_attention_mask is None
                 else expanded_attn_mask + combined_attention_mask
@@ -311,7 +312,7 @@ class InternVLModel(MultiModalModule):
         shift_logits = shift_logits.view(-1, self.vocab_size)
         shift_labels = shift_labels.view(-1)
 
-        shift_labels = shift_labels.to(shift_logits.device)
+        shift_labels = shift_labels
         loss = loss_fct(shift_logits, shift_labels)
         if ignore_flag:
             loss = loss * 0.0
diff --git a/mindspeed_mm/models/predictor/dits/__init__.py b/mindspeed_mm/models/predictor/dits/__init__.py
index a834c2d..8f3be8a 100644
--- a/mindspeed_mm/models/predictor/dits/__init__.py
+++ b/mindspeed_mm/models/predictor/dits/__init__.py
@@ -4,6 +4,6 @@ from .latte import Latte
 from .stdit import STDiT
 from .stdit3 import STDiT3
 from .sat_dit import SatDiT
-from .pt_dit_diffusers import PTDiTDiffuser as PTDiT
+# from .pt_dit_diffusers import PTDiTDiffuser as PTDiT
 
-__all__ = ["VideoDiT", "VideoDitSparse", "Latte", "STDiT", "STDiT3", "SatDiT", "VideoDitSparseI2V", "PTDiT"]
+__all__ = ["VideoDiT", "VideoDitSparse", "Latte", "STDiT", "STDiT3", "SatDiT", "VideoDitSparseI2V"]
diff --git a/mindspeed_mm/models/predictor/dits/pt_dit_diffusers.py b/mindspeed_mm/models/predictor/dits/pt_dit_diffusers.py
index 9c99d9a..0fc389e 100644
--- a/mindspeed_mm/models/predictor/dits/pt_dit_diffusers.py
+++ b/mindspeed_mm/models/predictor/dits/pt_dit_diffusers.py
@@ -23,26 +23,26 @@ import torch_npu
 import torch
 import torch.nn.functional as F
 import torch.nn.init as init
-from diffusers.configuration_utils import ConfigMixin, register_to_config
-from diffusers.models.embeddings import Timesteps, TimestepEmbedding
-from diffusers.models.modeling_utils import ModelMixin
-from diffusers.models.normalization import AdaLayerNormSingle
-from diffusers.models.attention import AdaLayerNorm, FeedForward
-from diffusers.models.attention_processor import Attention
-
-from diffusers.utils import BaseOutput, is_torch_version
+# from diffusers.configuration_utils import ConfigMixin, register_to_config
+# from diffusers.models.embeddings import Timesteps, TimestepEmbedding
+# from diffusers.models.modeling_utils import ModelMixin
+# from diffusers.models.normalization import AdaLayerNormSingle
+# from diffusers.models.attention import AdaLayerNorm, FeedForward
+# from diffusers.models.attention_processor import Attention
+
+# from diffusers.utils import BaseOutput, is_torch_version
 from einops import rearrange, repeat
 from torch import nn
-from diffusers.utils.torch_utils import maybe_allow_in_graph
+# from diffusers.utils.torch_utils import maybe_allow_in_graph
 
 from mindspeed_mm.models.common.embeddings import PatchEmbed2D_3DsincosPE
 
 
-try:
-    from diffusers.models.embeddings import PixArtAlphaTextProjection
-except ImportError:
-    from diffusers.models.embeddings import \
-        CaptionProjection as PixArtAlphaTextProjection
+# try:
+#     from diffusers.models.embeddings import PixArtAlphaTextProjection
+# except ImportError:
+#     from diffusers.models.embeddings import \
+#         CaptionProjection as PixArtAlphaTextProjection
 
 
 def zero_module(module):
@@ -51,7 +51,7 @@ def zero_module(module):
     return module
 
 
-@maybe_allow_in_graph
+# @maybe_allow_in_graph
 class ProxyTokensTransformerBlock(nn.Module):
     r"""
     Parameters:
diff --git a/mindspeed_mm/models/predictor/dits/sat_dit.py b/mindspeed_mm/models/predictor/dits/sat_dit.py
index 820b2e0..1d641fe 100644
--- a/mindspeed_mm/models/predictor/dits/sat_dit.py
+++ b/mindspeed_mm/models/predictor/dits/sat_dit.py
@@ -4,7 +4,7 @@ from typing import Optional, Tuple, Dict
 from contextlib import nullcontext
 
 import torch
-from diffusers.models.embeddings import SinusoidalPositionalEmbedding
+# from diffusers.models.embeddings import SinusoidalPositionalEmbedding
 from einops import rearrange
 from megatron.core import mpu, tensor_parallel
 from megatron.training import get_args
@@ -128,6 +128,16 @@ class SatDiT(MultiModalModule):
         else:
             self.patch_embed = VideoPatch2D(in_channels, inner_dim, self.patch_size_h)
 
+        # self.pos_embed = Rotary3DPositionEmbedding(
+        #     hidden_size_head=head_dim,
+        #     text_length=text_length,
+        #     height=input_size[1] // self.patch_size_h,
+        #     width=input_size[2] // self.patch_size_w,
+        #     compressed_num_frames=(input_size[0] - 1) // interpolation_scale[0] + 1,
+        #     hidden_size=inner_dim,
+        #     learnable_pos_embed=learnable_pos_embed
+        # )
+
         self.pos_embed = Rotary3DPositionEmbedding(
             hidden_size_head=head_dim,
             text_length=text_length,
@@ -135,8 +145,22 @@ class SatDiT(MultiModalModule):
             width=input_size[2] // self.patch_size_w,
             compressed_num_frames=(input_size[0] - 1) // interpolation_scale[0] + 1,
             hidden_size=inner_dim,
-            learnable_pos_embed=learnable_pos_embed
+            learnable_pos_embed=False#learnable_pos_embed
         )
+
+        self.learnable_pos_embed=learnable_pos_embed
+        if  self.learnable_pos_embed:
+
+            height=input_size[1] // self.patch_size_h
+            width=input_size[2] // self.patch_size_w
+            compressed_num_frames=(input_size[0] - 1) // interpolation_scale[0] + 1
+            hidden_size=inner_dim
+            num_patches = int(height * width * compressed_num_frames + text_length)
+            self.text_length_tmp = text_length
+            self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches, int(hidden_size)), requires_grad=True)
+        else:
+            self.pos_embedding = None
+
         # Init VideoDiTBlock
         self.videodit_blocks = nn.ModuleList(
             [
@@ -244,7 +268,7 @@ class SatDiT(MultiModalModule):
         height, width = latents.shape[-2] // self.patch_size_h, latents.shape[-1] // self.patch_size_w
 
         if "masked_video" in kwargs.keys() and kwargs["masked_video"] is not None:
-            latents = torch.cat([latents, kwargs["masked_video"]], dim=1)
+            latents = torch.cat([latents, kwargs["masked_video"].to(latents.dtype)], dim=1)
 
         added_cond_kwargs = {"resolution": None, "aspect_ratio": None}
         latents_vid, latents_img, prompt_vid, prompt_img, timestep_vid, timestep_img, \
@@ -425,8 +449,15 @@ class SatDiT(MultiModalModule):
                                                         rope_H=h // self.patch_size[1],
                                                         rope_W=w // self.patch_size[2])
             _, seq_len, _ = latents_vid.shape
-            pos_emb = self.pos_embed.position_embedding_forward(latents.to(self.dtype),
-                                                                seq_length=seq_len - self.text_length)
+            #megatron sync parameter in forward only
+
+            if self.learnable_pos_embed:
+                seq_length=seq_len - self.text_length_tmp
+                pos_emb = self.pos_embedding[:, :self.text_length_tmp + seq_length]
+            else:
+                pos_emb = self.pos_embed.position_embedding_forward(latents.to(self.dtype),
+                        seq_length=seq_len - self.text_length)
+
             if pos_emb is not None:
                 latents_vid = latents_vid + pos_emb
         else:
@@ -479,7 +510,7 @@ class SatDiT(MultiModalModule):
 
         # unpatchify
         output = rearrange(latents, "b (t h w) (c o p q) -> b (t o) c (h p) (w q)",
-                           b=latents.shape[0], h=height, w=width,
+                           b=latents.shape[0], h=height.item(), w=width.item(),
                            o=self.patch_size_t, p=self.patch_size_h, q=self.patch_size_w,
                            c=self.out_channels).transpose(1, 2)
         return output
diff --git a/mindspeed_mm/models/predictor/dits/stdit.py b/mindspeed_mm/models/predictor/dits/stdit.py
index d11c38e..3583d68 100644
--- a/mindspeed_mm/models/predictor/dits/stdit.py
+++ b/mindspeed_mm/models/predictor/dits/stdit.py
@@ -2,8 +2,8 @@ import numpy as np
 import torch
 import torch.nn as nn
 from einops import rearrange
-from timm.models.layers import DropPath
-from timm.models.vision_transformer import Mlp
+# from timm.models.layers import DropPath
+# from timm.models.vision_transformer import Mlp
 from megatron.core import mpu, tensor_parallel
 from megatron.training import get_args
 
diff --git a/mindspeed_mm/models/predictor/dits/stdit3.py b/mindspeed_mm/models/predictor/dits/stdit3.py
index 5629496..bd1638e 100644
--- a/mindspeed_mm/models/predictor/dits/stdit3.py
+++ b/mindspeed_mm/models/predictor/dits/stdit3.py
@@ -4,8 +4,8 @@ import torch_npu
 import torch.distributed as dist
 import torch.nn as nn
 from einops import rearrange
-from timm.models.layers import DropPath
-from timm.models.vision_transformer import Mlp
+# from timm.models.layers import DropPath
+# from timm.models.vision_transformer import Mlp
 
 from megatron.core import mpu
 from mindspeed_mm.models.common.module import MultiModalModule
diff --git a/mindspeed_mm/models/predictor/dits/video_dit.py b/mindspeed_mm/models/predictor/dits/video_dit.py
index b16dfd0..868cc5e 100644
--- a/mindspeed_mm/models/predictor/dits/video_dit.py
+++ b/mindspeed_mm/models/predictor/dits/video_dit.py
@@ -4,9 +4,9 @@ from einops import rearrange, repeat
 import torch
 from torch import nn
 import torch.nn.functional as F
-from diffusers.models.embeddings import SinusoidalPositionalEmbedding, PixArtAlphaTextProjection
-from diffusers.models.normalization import AdaLayerNorm, AdaLayerNormContinuous, AdaLayerNormZero, AdaLayerNormSingle
-from diffusers.models.attention import FeedForward
+# from diffusers.models.embeddings import SinusoidalPositionalEmbedding, PixArtAlphaTextProjection
+# from diffusers.models.normalization import AdaLayerNorm, AdaLayerNormContinuous, AdaLayerNormZero, AdaLayerNormSingle
+# from diffusers.models.attention import FeedForward
 from megatron.core import mpu, tensor_parallel
 from megatron.training import get_args
 
diff --git a/mindspeed_mm/models/predictor/dits/video_dit_sparse.py b/mindspeed_mm/models/predictor/dits/video_dit_sparse.py
index 4e980ad..9ec2ca5 100644
--- a/mindspeed_mm/models/predictor/dits/video_dit_sparse.py
+++ b/mindspeed_mm/models/predictor/dits/video_dit_sparse.py
@@ -5,8 +5,8 @@ from einops import rearrange, repeat
 import torch
 from torch import nn
 import torch.nn.functional as F
-from diffusers.models.embeddings import PixArtAlphaTextProjection
-from diffusers.models.normalization import AdaLayerNormSingle
+# from diffusers.models.embeddings import PixArtAlphaTextProjection
+# from diffusers.models.normalization import AdaLayerNormSingle
 from megatron.core import mpu, tensor_parallel
 from megatron.training import get_args
 
diff --git a/mindspeed_mm/models/predictor/predict_model.py b/mindspeed_mm/models/predictor/predict_model.py
index 83fc772..54fcf73 100644
--- a/mindspeed_mm/models/predictor/predict_model.py
+++ b/mindspeed_mm/models/predictor/predict_model.py
@@ -2,7 +2,7 @@ from torch import nn
 from megatron.training.utils import print_rank_0
 
 from mindspeed_mm.models.common.checkpoint import load_checkpoint
-from .dits import VideoDiT, Latte, STDiT, STDiT3, VideoDitSparse, SatDiT, VideoDitSparseI2V, PTDiT
+from .dits import VideoDiT, Latte, STDiT, STDiT3, VideoDitSparse, SatDiT, VideoDitSparseI2V
 
 PREDICTOR_MODEL_MAPPINGS = {
     "videodit": VideoDiT,
@@ -12,7 +12,6 @@ PREDICTOR_MODEL_MAPPINGS = {
     "stdit": STDiT,
     "stdit3": STDiT3,
     "satdit": SatDiT,
-    "ptdit": PTDiT,
 }
 
 
diff --git a/mindspeed_mm/models/qwen2vl_model.py b/mindspeed_mm/models/qwen2vl_model.py
index 6226377..71dbfc6 100644
--- a/mindspeed_mm/models/qwen2vl_model.py
+++ b/mindspeed_mm/models/qwen2vl_model.py
@@ -297,7 +297,11 @@ class Qwen2VLModel(MultiModalModule):
                 input_embeds = input_embeds.transpose(0, 1)
                 image_mask = torch.eq(input_ids, self.img_context_token_id).unsqueeze(-1).expand_as(input_embeds)
                 vit_embeds = vit_embeds[:, 0, :]
+                orig_dtype = vit_embeds.dtype
+                input_embeds = input_embeds.to(torch.float32)
+                vit_embeds = vit_embeds.to(torch.float32)
                 input_embeds = input_embeds.masked_scatter(image_mask, vit_embeds)
+                input_embeds = input_embeds.to(orig_dtype)
                 input_embeds = input_embeds.transpose(0, 1).clone()
 
             past_seen_tokens = 0
diff --git a/mindspeed_mm/models/text_encoder/text_encoder.py b/mindspeed_mm/models/text_encoder/text_encoder.py
index 484fc9e..3a643d5 100644
--- a/mindspeed_mm/models/text_encoder/text_encoder.py
+++ b/mindspeed_mm/models/text_encoder/text_encoder.py
@@ -46,6 +46,7 @@ class TextEncoder(nn.Module):
         # Only huggingface backend is supported, OpenMind backend will be supported soon.
         module = importlib.import_module("transformers")
         automodel = getattr(module, self.automodel_name)
+        config["low_cpu_mem_usage"] = False
         self.model = automodel.from_pretrained(**config)
 
     def get_model(self):
diff --git a/mindspeed_mm/models/vision/vision_encoders/clip_vit_model.py b/mindspeed_mm/models/vision/vision_encoders/clip_vit_model.py
index 30d7df9..f3b1ce3 100644
--- a/mindspeed_mm/models/vision/vision_encoders/clip_vit_model.py
+++ b/mindspeed_mm/models/vision/vision_encoders/clip_vit_model.py
@@ -40,7 +40,7 @@ class CLIPViT(MultiModalModule):
         **kwargs,
     ) -> None:
         super().__init__(config=config)
-        self.device = get_device(config.device)
+        self.device = config.device
         self.class_token_len = config.class_token_len
         self.visual_hidden_size = config.hidden_size
         self.patch_size = config.patch_size
@@ -137,7 +137,7 @@ class CLIPViT(MultiModalModule):
         if attention_mask is None:
             attention_mask = torch.ones(
                 1, 1, self.seq_length, self.seq_length
-            ).to(self.device)  # [1, 1, s, s]
+            )  # [1, 1, s, s]
             attention_mask = attention_mask < 0.5  # to bool
 
         x = self.decoder(x, attention_mask)
diff --git a/mindspeed_mm/models/vision/vision_encoders/internvit_model.py b/mindspeed_mm/models/vision/vision_encoders/internvit_model.py
index 4b70577..15dcd38 100644
--- a/mindspeed_mm/models/vision/vision_encoders/internvit_model.py
+++ b/mindspeed_mm/models/vision/vision_encoders/internvit_model.py
@@ -11,7 +11,7 @@ from torch import nn
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import torch_npu
-from timm.models.layers import DropPath
+# from timm.models.layers import DropPath
 
 from megatron.core.transformer.attention import SelfAttention, SelfAttentionSubmodules
 from megatron.core.transformer.enums import AttnMaskType
diff --git a/mindspeed_mm/models/vision/vision_encoders/qwen2vl_vit_model.py b/mindspeed_mm/models/vision/vision_encoders/qwen2vl_vit_model.py
index aef68f0..2a8ba76 100644
--- a/mindspeed_mm/models/vision/vision_encoders/qwen2vl_vit_model.py
+++ b/mindspeed_mm/models/vision/vision_encoders/qwen2vl_vit_model.py
@@ -43,8 +43,10 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim
     if use_fused_rope:
         import torch_npu
         cos, sin = cos[:1], sin[:1]
-        q_embed = torch_npu.npu_rotary_mul(q, cos, sin)
-        k_embed = torch_npu.npu_rotary_mul(k, cos, sin)
+        #q_embed = torch_npu.npu_rotary_mul(q, cos, sin)
+        #k_embed = torch_npu.npu_rotary_mul(k, cos, sin)
+        q_embed = torch_npu.npu_rotary_position_embedding(q, cos, sin)
+        k_embed = torch_npu.npu_rotary_position_embedding(k, cos, sin)
     else:
         q_embed = (q * cos) + (rotate_half(q) * sin)
         k_embed = (k * cos) + (rotate_half(k) * sin)
@@ -61,7 +63,8 @@ def apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor, use_f
     sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()
     if use_fused_rope:
         import torch_npu
-        output = torch_npu.npu_rotary_mul(tensor, cos, sin).to(orig_dtype)
+        #output = torch_npu.npu_rotary_mul(tensor, cos, sin).to(orig_dtype)
+        output = torch_npu.npu_rotary_position_embedding(t, cos, sin).to(orig_dtype)
     else:
         output = ((tensor * cos) + (rotate_half(tensor) * sin)).to(orig_dtype)
     return output
@@ -78,7 +81,7 @@ class Qwen2VLRotaryEmbedding_llm(Qwen2VLRotaryEmbedding):
 
         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)
         position_ids_expanded = position_ids[:, :, None, :].float()
-        device_type = x_device.type
+        device_type = x_device#.type
         device_type = device_type if isinstance(device_type, str) and device_type != "mps" else "cpu"
         with torch.autocast(device_type=device_type, enabled=False):
             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(2, 3)
@@ -290,8 +293,8 @@ class VisionRotaryEmbedding(nn.Module):
         self.theta = theta
 
     def forward(self, seqlen: int) -> torch.Tensor:
-        inv_freq = 1.0 / (self.theta ** (torch.arange(0, self.dim, 2, dtype=torch.bfloat16) / self.dim)).to(
-            self.inv_freq.device)
+        inv_freq = 1.0 / (self.theta ** (torch.arange(0, self.dim, 2, dtype=torch.bfloat16) / self.dim))
+        #.to(self.inv_freq.device)
         self.register_buffer("inv_freq", inv_freq, persistent=False)
         seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=torch.bfloat16)
         freqs = torch.outer(seq, self.inv_freq)
@@ -432,7 +435,7 @@ class Qwen2VLViT(MultiModalModule):
 
         seq_len = images.shape[0]
         attention_mask = torch.full(
-            [1, seq_len, seq_len], torch.finfo(images.dtype).min, device=images.device,
+            [1, seq_len, seq_len], -3.3895313892515355e+38, device=images.device,
             dtype=torch.bool
         )
         for i in range(1, len(cu_seqlens)):
diff --git a/mindspeed_mm/models/vl_model.py b/mindspeed_mm/models/vl_model.py
index a96b02a..7ce001f 100644
--- a/mindspeed_mm/models/vl_model.py
+++ b/mindspeed_mm/models/vl_model.py
@@ -315,7 +315,7 @@ class VLModel(MultiModalModule):
             shift_logits = shift_logits.view(-1, self.text_decoder.vocab_size)
             shift_labels = shift_labels.view(-1)
             # Enable model parallelism
-            shift_labels = shift_labels.to(shift_logits.device)
+            shift_labels = shift_labels
             loss = loss_fct(shift_logits, shift_labels)
 
         return loss
@@ -326,4 +326,4 @@ def _load_checkpoint(model, ckpt_path):
         load_params = torch.load(ckpt_path, map_location="cpu")
         print(model.load_state_dict(load_params, strict=False))
     else:
-        print("Warning: ckpt path is None or empty, skipping loading ckpt.")
\ No newline at end of file
+        print("Warning: ckpt path is None or empty, skipping loading ckpt.")
diff --git a/mindspeed_mm/patchs/diffusers_patches.py b/mindspeed_mm/patchs/diffusers_patches.py
index 96e1961..d259f1b 100644
--- a/mindspeed_mm/patchs/diffusers_patches.py
+++ b/mindspeed_mm/patchs/diffusers_patches.py
@@ -14,8 +14,8 @@
 # limitations under the License.
 
 import torch_npu
-from diffusers.utils.deprecation_utils import deprecate
-from diffusers.utils.import_utils import is_torch_npu_available
+# from diffusers.utils.deprecation_utils import deprecate
+# from diffusers.utils.import_utils import is_torch_npu_available
 
 
 def geglu_forward(self, hidden_states, *args, **kwargs):
diff --git a/mindspeed_mm/tasks/__init__.py b/mindspeed_mm/tasks/__init__.py
index 701e37a..382e9ca 100644
--- a/mindspeed_mm/tasks/__init__.py
+++ b/mindspeed_mm/tasks/__init__.py
@@ -1,3 +1,3 @@
-from mindspeed_mm.tasks.inference import sora_pipeline_dict, vlm_pipeline_dict
+# from mindspeed_mm.tasks.inference import sora_pipeline_dict, vlm_pipeline_dict
 
-__all__ = ["sora_pipeline_dict", "vlm_pipeline_dict"]
+# __all__ = ["sora_pipeline_dict", "vlm_pipeline_dict"]
diff --git a/mindspeed_mm/training.py b/mindspeed_mm/training.py
index db52bd8..78cc2a9 100644
--- a/mindspeed_mm/training.py
+++ b/mindspeed_mm/training.py
@@ -108,8 +108,8 @@ def pretrain(
     if args.log_progress:
         append_to_progress_log("Starting job")
 
-    torch.backends.cuda.matmul.allow_tf32 = getattr(args.mm.model, "allow_tf32", False)
-    torch.npu.config.allow_internal_format = getattr(args.mm.model, "allow_internal_format", False)
+    # torch.backends.cuda.matmul.allow_tf32 = getattr(args.mm.model, "allow_tf32", False)
+    # torch.npu.config.allow_internal_format = getattr(args.mm.model, "allow_internal_format", False)
 
     # Set pytorch JIT layer fusion options and warmup JIT functions.
     set_jit_fusion_options()
diff --git a/mindspeed_mm/utils/extra_processor/cogvideox_i2v_processor.py b/mindspeed_mm/utils/extra_processor/cogvideox_i2v_processor.py
index 87ce289..3d3a21f 100644
--- a/mindspeed_mm/utils/extra_processor/cogvideox_i2v_processor.py
+++ b/mindspeed_mm/utils/extra_processor/cogvideox_i2v_processor.py
@@ -24,7 +24,7 @@ class CogVideoXI2VProcessor:
         self.noised_image_input = config.get("noised_image_input", True)
 
     def add_noise_to_image(self, image):
-        sigma = torch.normal(mean=-3.0, std=0.5, size=(image.shape[0],)).to(image.device)
+        sigma = torch.normal(mean=-3.0, std=0.5, size=(image.shape[0],))
         sigma = torch.exp(sigma).to(image.dtype)
         image_noise = torch.randn_like(image) * sigma[:, None, None, None, None]
         image = image + image_noise
diff --git a/mindspeed_mm/utils/utils.py b/mindspeed_mm/utils/utils.py
index beae077..a40310d 100644
--- a/mindspeed_mm/utils/utils.py
+++ b/mindspeed_mm/utils/utils.py
@@ -66,7 +66,7 @@ def get_device(device="npu"):
 
 def get_dtype(dtype):
     """return torch type according to the string"""
-    if isinstance(dtype, torch.dtype):
+    if dtype == torch.bfloat16 or dtype == torch.float32:
         return dtype
     dtype_mapping = {
         "int32": torch.int32,
@@ -100,7 +100,7 @@ def cast_tuple(t, length=1):
 
 
 def quick_gelu(x: torch.Tensor) -> torch.Tensor:
-    return x * torch.sigmoid(1.702 * x)
+    return x * torch.sigmoid(torch.Tensor([1.702], dtype=x.dtype) * x)
 
 
 _CONTEXT_PARALLEL_GROUP = None
diff --git a/pretrain_internvl.py b/pretrain_internvl.py
index bc5abc6..af16a19 100644
--- a/pretrain_internvl.py
+++ b/pretrain_internvl.py
@@ -7,7 +7,7 @@ import torch
 import torch.distributed
 import torch.nn.functional as F
 import mindspeed.megatron_adaptor
-from mindspeed.utils import get_batch_on_this_cp_rank
+# from mindspeed.utils import get_batch_on_this_cp_rank
 
 from megatron.core import mpu
 from megatron.core.enums import ModelType
@@ -52,11 +52,11 @@ def get_batch_on_this_tp_rank(data_iterator):
         else:
             batch = None
 
-        input_ids = batch['input_ids'].to(torch.cuda.current_device())
-        labels = batch['labels'].to(torch.cuda.current_device())
-        attention_mask = batch['attention_mask'].to(torch.cuda.current_device())
-        image = batch['pixel_values'].to(torch.cuda.current_device())
-        image_flags = batch['image_flags'].to(torch.cuda.current_device())
+        input_ids = batch['input_ids']
+        labels = batch['labels']
+        attention_mask = batch['attention_mask']
+        image = batch['pixel_values']
+        image_flags = batch['image_flags']
         _broadcast(input_ids)
         _broadcast(labels)
         _broadcast(attention_mask)
@@ -83,11 +83,11 @@ def get_batch(data_iterator):
         batch = next(data_iterator)
     else:
         raise ValueError("Data iterator is None. Unable to retrieve batch.")
-    input_ids = batch['input_ids'].to(torch.cuda.current_device())
-    labels = batch['labels'].to(torch.cuda.current_device())
-    attention_mask = batch['attention_mask'].to(torch.cuda.current_device())
-    image = batch['pixel_values'].to(torch.cuda.current_device())
-    image_flags = batch['image_flags'].to(torch.cuda.current_device())
+    input_ids = batch['input_ids']
+    labels = batch['labels']
+    attention_mask = batch['attention_mask']
+    image = batch['pixel_values']
+    image_flags = batch['image_flags']
     batch = {
         'input_ids': input_ids,
         'labels': labels,
diff --git a/pretrain_llava.py b/pretrain_llava.py
index 37eb005..b84f4c6 100644
--- a/pretrain_llava.py
+++ b/pretrain_llava.py
@@ -46,10 +46,10 @@ def get_batch(data_iterator):
         data = next(data_iterator)
     else:
         data = None
-    images = data["pixel_values"].to(dtype=torch.bfloat16, device=torch.cuda.current_device())
-    input_ids = data["input_ids"].to(device=torch.cuda.current_device())
-    labels = data["labels"].to(device=torch.cuda.current_device())
-    attention_mask = data["attention_mask"].to(device=torch.cuda.current_device())
+    images = data["pixel_values"]
+    input_ids = data["input_ids"]
+    labels = data["labels"]
+    attention_mask = data["attention_mask"]
 
     return images, input_ids, labels, attention_mask
 
diff --git a/pretrain_qwen2vl.py b/pretrain_qwen2vl.py
index 98d217b..9184052 100644
--- a/pretrain_qwen2vl.py
+++ b/pretrain_qwen2vl.py
@@ -45,11 +45,11 @@ def get_batch(data_iterator):
         batch = next(data_iterator)
     else:
         raise ValueError("Data iterator is None. Unable to retrieve batch.")
-    input_ids = batch['input_ids'].to(torch.cuda.current_device())
-    labels = batch['labels'].to(torch.cuda.current_device())
-    attention_mask = batch['attention_mask'].to(torch.cuda.current_device())
-    pixel_values = batch['pixel_values'].to(torch.cuda.current_device())
-    image_grid_thw = batch['image_grid_thw'].to(torch.cuda.current_device())
+    input_ids = batch['input_ids']#.to(torch.cuda.current_device())
+    labels = batch['labels']#.to(torch.cuda.current_device())
+    attention_mask = batch['attention_mask']#.to(torch.cuda.current_device())
+    pixel_values = batch['pixel_values']#.to(torch.cuda.current_device())
+    image_grid_thw = batch['image_grid_thw']#.to(torch.cuda.current_device())
     batch = {
         'input_ids': input_ids,
         'labels': labels,
diff --git a/pretrain_sora.py b/pretrain_sora.py
index 5abd459..3dd759d 100644
--- a/pretrain_sora.py
+++ b/pretrain_sora.py
@@ -48,9 +48,9 @@ def get_batch_on_this_tp_rank(data_iterator):
         batch = next(data_iterator)
     else:
         return None
-    for k, v in batch.items():
-        if isinstance(v, torch.Tensor):
-            batch[k] = v.to(torch.cuda.current_device())
+    # for k, v in batch.items():
+    #     if isinstance(v, torch.Tensor):
+    #         batch[k] = v.to(torch.cuda.current_device())
     return batch
 
 
