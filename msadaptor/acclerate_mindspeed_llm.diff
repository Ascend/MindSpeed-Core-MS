diff --git a/mindspeed/arguments.py b/mindspeed/arguments.py
index 2d2ca10..81fdb19 100644
--- a/mindspeed/arguments.py
+++ b/mindspeed/arguments.py
@@ -42,6 +42,7 @@ def process_args(parser):
     parser = _add_profile_args(parser)
     parser = _add_deepseek_args(parser)
     parser = _add_high_availability_args(parser)
+    parser = _add_self_defined_pipeline_args(parser)
 
     return parser
 
@@ -94,6 +95,18 @@ def _add_high_availability_args(parser):
                        help="high availability feature, enable hbmfault repair")
     return parser
 
+def _add_self_defined_pipeline_args(parser):
+    group = parser.add_argument_group(title='self_defined_pipeline')
+    group.add_argument('--pipeline-layer-index',
+                       type=str, default=None,
+                       help='self defined pipeline layer nums'
+                      )
+    group.add_argument('--pipeline-shapes',
+                       type=str, default=None,
+                       help='self defined pipeline shapes'
+                       )
+    return parser
+
 
 def _add_profile_args(parser):
     group = parser.add_argument_group(title='profile')
@@ -686,6 +699,13 @@ def validate_args_wrapper(validate_args):
                                          f'to 0 and smaller than args.num_layers({args.num_layers})')
                 noop_layers.add(int(x))
             args.noop_layers = noop_layers
+        if args.pipeline_layer_index:
+            if args.num_layer_list is not None:
+                raise AssertionError('--num-layer-list switch can not open with pipeline-layer-index')
+
+            import ast
+            args.pipeline_layer_index = ast.literal_eval(args.pipeline_layer_index)
+            args.pipeline_shapes = ast.literal_eval(args.pipeline_shapes)
 
         from megatron.training.arguments import _print_args
         _print_args('arguments', args, True)
diff --git a/mindspeed/core/context_parallel/adaptive_context_parallel.py b/mindspeed/core/context_parallel/adaptive_context_parallel.py
index 0f96438..166d39e 100644
--- a/mindspeed/core/context_parallel/adaptive_context_parallel.py
+++ b/mindspeed/core/context_parallel/adaptive_context_parallel.py
@@ -1,7 +1,7 @@
 # Copyright (c) 2024, Huawei Technologies Co., Ltd. All rights reserved.
 import torch
 import torch_npu
-from mindspeed.ops.npu_ring_attention_update import npu_ring_attention_update
+# from mindspeed.ops.npu_ring_attention_update import npu_ring_attention_update
 
 
 def flash_attn_p2p_communicate(scheduling_info, send_q_dst, recv_q_src, send_kv_dst, recv_kv_src, cp_group, rank):
diff --git a/mindspeed/core/context_parallel/ring_context_parallel.py b/mindspeed/core/context_parallel/ring_context_parallel.py
index 4634f34..3c97b03 100644
--- a/mindspeed/core/context_parallel/ring_context_parallel.py
+++ b/mindspeed/core/context_parallel/ring_context_parallel.py
@@ -2,11 +2,10 @@
 # Copyright (c) 2024, Huawei Technologies Co., Ltd.  All rights reserved.
 import torch
 import torch_npu
-from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention, npu_fusion_attention_grad
+# from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention, npu_fusion_attention_grad
 from .utils import RingP2P, causal_out_update, general_out_update, forward_update
 
 
-
 def causal_forward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_mask=None):
     cur_attn_mask = None
     if q_block_id == kv_block_id:
@@ -23,19 +22,19 @@ def causal_forward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_mask=Non
         cur_q = q[1]
         # [2, s, b, h] -> [2s, b, h]
         cur_k, cur_v = [x.view(-1, *x.shape[2:]) for x in [cur_k, cur_v]]
-    
+
     return cur_q, cur_k, cur_v, cur_attn_mask
 
 
-def causal_backward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_out, dout, 
+def causal_backward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_out, dout,
                           softmax_max, softmax_sum, attn_mask=None):
     cur_attn_mask = None
     if q_block_id >= kv_block_id:
         # [b, n, 2, s, 8] -> [b, n, 2s, 8]
         cur_softmax_max = softmax_max.view(softmax_max.shape[0], softmax_max.shape[1], -1,
-                                            softmax_max.shape[-1])
+                                           softmax_max.shape[-1])
         cur_softmax_sum = softmax_sum.view(softmax_sum.shape[0], softmax_sum.shape[1], -1,
-                                            softmax_sum.shape[-1])
+                                           softmax_sum.shape[-1])
         # [2, s, b, h] -> [2s, b, h]
         cur_q, cur_attn_out, cur_dout = [x.view(-1, *x.shape[2:]) for x in [q, attn_out, dout]]
         if q_block_id == kv_block_id:
@@ -50,7 +49,7 @@ def causal_backward_fetch(q_block_id, kv_block_id, q, cur_k, cur_v, attn_out, do
         # only q[1] attn_out[1] and dout[1] need to be calculated
         cur_q, cur_attn_out, cur_dout = [x[1] for x in [q, attn_out, dout]]
         cur_softmax_max, cur_softmax_sum = [x[:, :, 1, :, :] for x in [softmax_max, softmax_sum]]
-    
+
     return cur_q, cur_k, cur_v, cur_attn_out, cur_dout, cur_softmax_max, cur_softmax_sum, cur_attn_mask
 
 
@@ -69,18 +68,18 @@ def causal_grad_update(q_block_id, kv_block_id, cur_dq, cur_dk, cur_dv, dq, dk,
         dv[0].add_(cur_dv)
     else:
         dq[1].add_(cur_dq)
-        cur_dk = cur_dk.view(dk.shape) # [2s, b, h] -> [2, s, b, h]
+        cur_dk = cur_dk.view(dk.shape)  # [2s, b, h] -> [2, s, b, h]
         cur_dv = cur_dv.view(dv.shape)
         dk.add_(cur_dk)
         dv.add_(cur_dv)
-    
+
     return dq, dk, dv
 
 
 def cal_row(cur_q, cur_k, cur_v, s, attn_info):
     # q: [s, b, h], kv: [2s, b, h]
     n, pse, pse_type, attn_mask, softmax_scale, keep_prob, \
-    q_index_list, kv_index_list = attn_info
+        q_index_list, kv_index_list = attn_info
 
     # r1c0
     cur_attn_mask = None
@@ -130,7 +129,7 @@ def cal_row(cur_q, cur_k, cur_v, s, attn_info):
 
 def flash_attention_with_alibi_pse(q_block_id, kv_block_id, cur_qkv, attn_info, s):
     n, pse, pse_type, cur_attn_mask, softmax_scale, keep_prob, \
-    q_index_list, kv_index_list = attn_info
+        q_index_list, kv_index_list = attn_info
     cur_q, cur_k, cur_v = cur_qkv
     if q_block_id == kv_block_id:
         attn_outs_r0c0 = npu_fusion_attention(
@@ -196,7 +195,7 @@ def flash_attention_with_alibi_pse(q_block_id, kv_block_id, cur_qkv, attn_info,
 def cal_row_grad(cur_q, cur_k, cur_v, cur_dout, cur_softmax_max, cur_softmax_sum, cur_attn_out,
                  attn_grad_info, s, kv_block_id):
     n, pse, pse_type, attn_mask, softmax_scale, keep_prob, rng_states, \
-    q_index_list, kv_index_list = attn_grad_info
+        q_index_list, kv_index_list = attn_grad_info
 
     cur_attn_mask = None
     attn_grad_outs_r1c0 = npu_fusion_attention_grad(
@@ -248,7 +247,7 @@ def cal_row_grad(cur_q, cur_k, cur_v, cur_dout, cur_softmax_max, cur_softmax_sum
 def flash_attention_with_alibi_pse_grad(q_block_id, kv_block_id, cur_qkv, cur_dout, cur_attn_out,
                                         cur_softmax_max, cur_softmax_sum, attn_grad_info, s):
     n, pse, pse_type, cur_attn_mask, softmax_scale, keep_prob, rng_states, \
-    q_index_list, kv_index_list = attn_grad_info
+        q_index_list, kv_index_list = attn_grad_info
     cur_q, cur_k, cur_v = cur_qkv
 
     if q_block_id == kv_block_id:
@@ -340,34 +339,38 @@ def flash_attention_with_alibi_pse_grad(q_block_id, kv_block_id, cur_qkv, cur_do
         attn_grad_outs.append(torch.cat([attn_grad_outs_r1c0[1], attn_grad_outs_r1c1[1]]))
         attn_grad_outs.append(torch.cat([attn_grad_outs_r1c0[2], attn_grad_outs_r1c1[2]]))
 
-
     return attn_grad_outs
 
 
+from mindspore import nn
 
 
-class AttentionWithCp(torch.autograd.Function):
+class AttentionWithCp(nn.Cell):
     """Attention implementation with context parallelism"""
 
-    
-    @staticmethod
-    def forward(ctx, q, k, v, n, cp_para, softmax_scale=None, attn_mask=None, dropout_p=0.,
-                actual_seq_qlen=None, actual_seq_kvlen=None):    
+    def __init__(self, cp_para):
+        super().__init__()
+        self.block_size = None
+        self.batch_size = None
+        self.cp_para = cp_para
+
+    def construct(self, q, k, v, n, softmax_scale=None, attn_mask=None, dropout_p=0.,
+                  actual_seq_qlen=None, actual_seq_kvlen=None):
         keep_prob = 1. - dropout_p
-        causal = cp_para['causal']
-        cp_group = cp_para.get("cp_group")
-        cp_size = cp_para.get("cp_size")
-        rank = cp_para.get("rank")
-        cp_global_ranks = cp_para.get("cp_global_ranks")
-        cp_group_for_send_recv_overlap = cp_para.get("cp_group_for_send_recv_overlap")
+        causal = self.cp_para['causal']
+        cp_group = self.cp_para.get("cp_group")
+        cp_size = self.cp_para.get("cp_size")
+        rank = self.cp_para.get("rank")
+        cp_global_ranks = self.cp_para.get("cp_global_ranks")
+        cp_group_for_send_recv_overlap = self.cp_para.get("cp_group_for_send_recv_overlap")
         # WARNING: Degrade to original ring attention, if ranks and comm groups for double ring are not provided
-        cp_inner_ranks = cp_para.get("cp_inner_ranks", [torch.distributed.get_rank()])
-        cp_outer_ranks = cp_para.get("cp_outer_ranks", cp_global_ranks)
-        cp_group_for_intra_window = cp_para.get('cp_group_for_intra_window')
-        cp_group_for_intra_window_send_recv_overlap = cp_para.get('cp_group_for_intra_window_send_recv_overlap')
+        cp_inner_ranks = self.cp_para.get("cp_inner_ranks", [torch.distributed.get_rank()])
+        cp_outer_ranks = self.cp_para.get("cp_outer_ranks", cp_global_ranks)
+        cp_group_for_intra_window = self.cp_para.get('cp_group_for_intra_window')
+        cp_group_for_intra_window_send_recv_overlap = self.cp_para.get('cp_group_for_intra_window_send_recv_overlap')
 
-        pse = cp_para.get("pse")
-        pse_type = cp_para.get("pse_type")
+        pse = self.cp_para.get("pse")
+        pse_type = self.cp_para.get("pse_type")
 
         inner_ring = RingP2P(cp_inner_ranks, cp_group_for_intra_window, cp_group_for_intra_window_send_recv_overlap)
         outer_ring = RingP2P(cp_outer_ranks, cp_group, cp_group_for_send_recv_overlap)
@@ -384,7 +387,7 @@ class AttentionWithCp(torch.autograd.Function):
             # split chunk[i]~chunk[cp_size-i-1] into chunk[i] and chunk[cp_size-i-1],, [2s, b, h] -> [2, s, b, h]
             q, k, v = [x.view(2, x.shape[0] // 2, *x.shape[1:]) for x in [q, k, v]]
 
-        cur_kv = torch.cat((k.unsqueeze(0), v.unsqueeze(0)), dim=0) # [2, 2, s, b, h]
+        cur_kv = torch.cat((k.unsqueeze(0), v.unsqueeze(0)), dim=0)  # [2, 2, s, b, h]
         next_kv = torch.empty_like(cur_kv)
         next_round_kv = torch.empty_like(cur_kv)
         attn_out, softmax_max, softmax_sum = None, None, None
@@ -403,10 +406,10 @@ class AttentionWithCp(torch.autograd.Function):
                 if i < inner_size - 1:
                     inner_ring.async_send_recv(send_tensor=cur_kv, recv_tensor=next_kv)
 
-                cur_k, cur_v = cur_kv[0], cur_kv[1] # [2, s, b, h]
+                cur_k, cur_v = cur_kv[0], cur_kv[1]  # [2, s, b, h]
                 if causal:
                     cur_q, cur_k, cur_v, cur_attn_mask = causal_forward_fetch(q_block_id, kv_block_id,
-                                                                            q, cur_k, cur_v, attn_mask)
+                                                                              q, cur_k, cur_v, attn_mask)
 
                     # flash attention forward
                     if pse is None:
@@ -440,7 +443,7 @@ class AttentionWithCp(torch.autograd.Function):
                     # [2s, b, h], [b, n, 2s, 8], [b, n, 2s, 8]
                     this_mask = AttentionWithCp.compute_mask(
                         actual_seq_qlen, actual_seq_kvlen,
-                        q_block_id, kv_block_id, 
+                        q_block_id, kv_block_id,
                         attn_mask
                     )
 
@@ -457,76 +460,80 @@ class AttentionWithCp(torch.autograd.Function):
                     )
 
                     global_attn_outs = general_out_update(q_block_id, kv_block_id, attn_outs, global_attn_outs)
-                
+
                 if inner_ring.wait():
-                    cur_kv, next_kv = next_kv, cur_kv # double buffer
+                    cur_kv, next_kv = next_kv, cur_kv  # double buffer
                     kv_block_id = (kv_block_id + inner_size - 1) % inner_size + kv_block_offset
 
             if outer_ring.wait():
-                cur_kv, next_round_kv = next_round_kv, cur_kv # double buffer
+                cur_kv, next_round_kv = next_round_kv, cur_kv  # double buffer
                 kv_block_id_outer = (kv_block_id_outer + cp_size - inner_size) % cp_size
 
-
-
         k, v = cur_kv[0], cur_kv[1]
         attn_out, softmax_max, softmax_sum, rng_states = global_attn_outs
         if causal:
             q, k, v = [x.view(-1, *x.shape[2:]) for x in [q, k, v]]
-        
+
         attn_mask = attn_mask if isinstance(attn_mask, list) else [attn_mask]
-        
-        ctx.save_for_backward(q, k, v, *attn_mask, attn_out, softmax_max, softmax_sum)
-        ctx.n = n
-        ctx.causal = causal
-        ctx.softmax_scale = softmax_scale
-        ctx.cp_group = cp_group
-        ctx.cp_size = cp_size
-        ctx.cp_rank = rank
-        ctx.cp_global_ranks = cp_global_ranks
-        ctx.cp_inner_ranks = cp_inner_ranks
-        ctx.cp_outer_ranks = cp_outer_ranks
-        ctx.cp_dkv_outer_ranks = cp_para.get('cp_dkv_outer_ranks', cp_global_ranks)
-        ctx.kv_block_id = kv_block_id
-        ctx.keep_prob = keep_prob
-        ctx.rng_states = rng_states
-        ctx.pse = pse
-        ctx.pse_type = pse_type
-        ctx.cp_group_for_send_recv_overlap = cp_group_for_send_recv_overlap
-        ctx.cp_group_for_intra_window = cp_group_for_intra_window
-        ctx.cp_group_for_intra_window_send_recv_overlap = cp_group_for_intra_window_send_recv_overlap
-        ctx.actual_seq_qlen = actual_seq_qlen
-        ctx.actual_seq_kvlen = actual_seq_kvlen
+
+        self.k = k
+        self.v = v
+        self.attn_mask = attn_mask
+        # save forward outputs
+        self.softmax_max = softmax_max
+        self.softmax_sum = softmax_sum
+        self.causal = causal
+        self.softmax_scale = softmax_scale
+        self.cp_group = cp_group
+        self.cp_size = cp_size
+        self.cp_rank = rank
+        self.cp_global_ranks = cp_global_ranks
+        self.cp_inner_ranks = cp_inner_ranks
+        self.cp_outer_ranks = cp_outer_ranks
+        self.cp_dkv_outer_ranks = self.cp_para.get('cp_dkv_outer_ranks', cp_global_ranks)
+        self.kv_block_id = kv_block_id
+        self.keep_prob = keep_prob
+        self.rng_states = rng_states
+        self.pse = pse
+        self.cp_group_for_send_recv_overlap = cp_group_for_send_recv_overlap
+        self.cp_group_for_intra_window = cp_group_for_intra_window
+        self.cp_group_for_intra_window_send_recv_overlap = cp_group_for_intra_window_send_recv_overlap
 
         return attn_out
 
-    @staticmethod
-    def backward(ctx, dout):
-        q, k, v, *attn_mask, attn_out, softmax_max, softmax_sum = ctx.saved_tensors
+    def bprop(self, q, k, v, n, softmax_scale, attn_mask, dropout_p, actual_seq_qlen, actual_seq_kvlen, attn_out, dout):
+        k = self.k
+        v = self.v
+        cp_para = self.cp_para
+        softmax_max = self.softmax_max
+        softmax_sum = self.softmax_sum
+        attn_mask = self.attn_mask
+
         if len(attn_mask) == 1:
             attn_mask = attn_mask[0]
 
-        n = ctx.n
-        causal = ctx.causal
-        softmax_scale = ctx.softmax_scale
-        cp_group = ctx.cp_group
-        cp_size = ctx.cp_size
-        rank = ctx.cp_rank
-        keep_prob = ctx.keep_prob
-        rng_states = ctx.rng_states
-        pse = ctx.pse
-        pse_type = ctx.pse_type
-        cp_group_for_send_recv_overlap = ctx.cp_group_for_send_recv_overlap
-        cp_group_for_intra_window = ctx.cp_group_for_intra_window
-        cp_group_for_intra_window_send_recv_overlap = ctx.cp_group_for_intra_window_send_recv_overlap
+        causal = self.causal
+        cp_group = self.cp_group
+        cp_size = self.cp_size
+        rank = self.cp_rank
+        keep_prob = self.keep_prob
+        rng_states = self.rng_states
+        pse = self.pse
+        cp_group_for_send_recv_overlap = self.cp_group_for_send_recv_overlap
+        cp_group_for_intra_window = self.cp_group_for_intra_window
+        cp_group_for_intra_window_send_recv_overlap = self.cp_group_for_intra_window_send_recv_overlap
+        cp_shape_order = cp_para.get("cp_shape_order", "SBH")
+
         # Reversed order of forward
-        inner_size = len(ctx.cp_inner_ranks)
-        outer_size = len(ctx.cp_outer_ranks)
-        
-        intra_kv_comm = RingP2P(ctx.cp_inner_ranks, cp_group_for_intra_window, cp_group_for_intra_window_send_recv_overlap, is_backward=True)
-        intra_dkv_comm = RingP2P(ctx.cp_inner_ranks, cp_group_for_intra_window, cp_group_for_intra_window_send_recv_overlap, is_backward=True)
-        inter_kv_comm = RingP2P(ctx.cp_outer_ranks, cp_group, cp_group_for_send_recv_overlap, is_backward=True)
-        inter_dkv_comm = RingP2P(ctx.cp_dkv_outer_ranks, cp_group, cp_group_for_send_recv_overlap, is_backward=True)
+        inner_size = len(self.cp_inner_ranks)
+        outer_size = len(self.cp_outer_ranks)
 
+        intra_kv_comm = RingP2P(self.cp_inner_ranks, cp_group_for_intra_window,
+                                cp_group_for_intra_window_send_recv_overlap, is_backward=True)
+        intra_dkv_comm = RingP2P(self.cp_inner_ranks, cp_group_for_intra_window,
+                                 cp_group_for_intra_window_send_recv_overlap, is_backward=True)
+        inter_kv_comm = RingP2P(self.cp_outer_ranks, cp_group, cp_group_for_send_recv_overlap, is_backward=True)
+        inter_dkv_comm = RingP2P(self.cp_dkv_outer_ranks, cp_group, cp_group_for_send_recv_overlap, is_backward=True)
 
         if causal:
             # split chunk[i]~chunk[cp_size-i-1] into chunk[i] and chunk[cp_size-i-1], [2s, b, h] -> [2, s, b, h]
@@ -553,7 +560,7 @@ class AttentionWithCp(torch.autograd.Function):
                         softmax_max=cur_softmax_max,
                         softmax_sum=cur_softmax_sum,
                         attention_in=cur_attn_out,
-                        scale_value=softmax_scale,
+                        scale=softmax_scale,
                         pre_tockens=cur_k.shape[0],
                         next_tockens=0 if cur_attn_mask is not None else cur_k.shape[0],
                         sparse_mode=3 if cur_attn_mask is not None else 0,
@@ -581,9 +588,9 @@ class AttentionWithCp(torch.autograd.Function):
             else:
                 this_mask = AttentionWithCp.compute_mask(
                     ctx.actual_seq_qlen, ctx.actual_seq_kvlen,
-                    q_block_id, kv_block_id, 
+                    q_block_id, kv_block_id,
                     attn_mask
-                )                
+                )
                 attn_grad_outs = torch_npu.npu_fusion_attention_grad(
                     q, cur_k, cur_v, dout, n,
                     "SBH",
@@ -603,11 +610,10 @@ class AttentionWithCp(torch.autograd.Function):
                     numels=rng_states[kv_block_id][2],
                 )
                 cur_dq, cur_dk, cur_dv = attn_grad_outs[0], attn_grad_outs[1], attn_grad_outs[2]
-            
-            return cur_dq, cur_dk, cur_dv
 
+            return cur_dq, cur_dk, cur_dv
 
-        cur_kv_dkv = torch.zeros((2, 2, *k.shape), dtype=k.dtype, device=k.device) # [2, 2, 2, s, b, h]
+        cur_kv_dkv = torch.zeros((2, 2, *k.shape), dtype=k.dtype, device=k.device)  # [2, 2, 2, s, b, h]
         cur_kv_dkv[0].copy_(torch.cat((k.unsqueeze(0), v.unsqueeze(0)), dim=0))
         next_kv_dkv = cur_kv_dkv.clone()
         next_round_kv_dkv = cur_kv_dkv.clone()
@@ -616,10 +622,9 @@ class AttentionWithCp(torch.autograd.Function):
         next_kv, next_dkv = next_kv_dkv[0], next_kv_dkv[1]
         next_round_kv, next_round_dkv = next_round_kv_dkv[0], next_round_kv_dkv[1]
 
-        q_block_id, kv_block_id, kv_block_id_outer = rank, ctx.kv_block_id, ctx.kv_block_id
+        q_block_id, kv_block_id, kv_block_id_outer = rank, self.kv_block_id, self.kv_block_id
 
-
-        dq = torch.zeros_like(q)# [2, s, b, h]
+        dq = torch.zeros_like(q)  # [2, s, b, h]
         for j in range(outer_size):
             kv_block_id = kv_block_id_outer
             kv_block_offset = (kv_block_id // inner_size) * inner_size
@@ -630,7 +635,6 @@ class AttentionWithCp(torch.autograd.Function):
             if j + 1 != outer_size:
                 inter_kv_comm.async_send_recv(send_tensor=cur_kv, recv_tensor=next_round_kv)
 
-
             for i in range(inner_size):
                 if i > 0:
                     intra_kv_comm.wait()
@@ -638,18 +642,18 @@ class AttentionWithCp(torch.autograd.Function):
 
                 if i + 1 != inner_size:
                     intra_kv_comm.async_send_recv(send_tensor=cur_kv, recv_tensor=next_kv)
-                
+
                 cur_k, cur_v = cur_kv[0], cur_kv[1]
 
                 dq_step, dk_step, dv_step = backward_step_helper(q_block_id, kv_block_id, q, cur_k, cur_v)
 
-                if i == 0 and j > 0: # receive dk dv from last window
+                if i == 0 and j > 0:  # receive dk dv from last window
                     inter_dkv_comm.wait()
                     cur_dkv, next_round_dkv = next_round_dkv, cur_dkv
-                elif i > 0: # receive dk dv from last step
+                elif i > 0:  # receive dk dv from last step
                     intra_dkv_comm.wait()
                     cur_dkv, next_dkv = next_dkv, cur_dkv
-                
+
                 dk, dv = cur_dkv[0], cur_dkv[1]
                 # update qkv grades
                 if causal:
@@ -677,7 +681,6 @@ class AttentionWithCp(torch.autograd.Function):
 
         dk, dv = cur_dkv[0], cur_dkv[1]
 
-
         # [2, s, b, h] -> [2s, b, h]
         if causal:
             dq, dk, dv = [x.view(-1, *x.shape[2:]) for x in [dq, dk, dv]]
@@ -694,41 +697,43 @@ class AttentionWithCp(torch.autograd.Function):
             seq_batch = [seq1d[indexes[i]:indexes[i + 1]] for i in range(len(indexes) - 1)]
             return [[elem - i * seq_len for elem in seq] for i, seq in enumerate(seq_batch)]
 
-        if actual_seq_qlen:  
+        if actual_seq_qlen:
             actual_seq_qlen = batch_index(actual_seq_qlen)
             actual_seq_kvlen = batch_index(actual_seq_kvlen)
             block_size = cls.block_size
             actual_seq_qlen = [[0] + lst for lst in actual_seq_qlen]
             sub_seq_qlen = [torch.tensor(x[1:]) - torch.tensor(x[:-1]) for x in actual_seq_qlen]
-            sub_seq_qid = torch.stack([torch.arange(len(lst)).repeat_interleave(lst) for lst in sub_seq_qlen]).npu() # B S
+            sub_seq_qid = torch.stack(
+                [torch.arange(len(lst)).repeat_interleave(lst) for lst in sub_seq_qlen]).npu()  # B S
 
             this_ids = sub_seq_qid[:, q_block_id * block_size:(q_block_id + 1) * block_size].npu()
-            this_tile = this_ids.unsqueeze(dim=2) # B S 1
+            this_tile = this_ids.unsqueeze(dim=2)  # B S 1
 
             actual_seq_kvlen = [[0] + lst for lst in actual_seq_kvlen]
             sub_seq_kvlen = [torch.tensor(x[1:]) - torch.tensor(x[:-1]) for x in actual_seq_kvlen]
-            sub_seq_kvid = torch.stack([torch.arange(len(lst)).repeat_interleave(lst) for lst in sub_seq_kvlen]).npu() # B S
+            sub_seq_kvid = torch.stack(
+                [torch.arange(len(lst)).repeat_interleave(lst) for lst in sub_seq_kvlen]).npu()  # B S
             other_ids = sub_seq_kvid[:, kv_block_id * block_size:(kv_block_id + 1) * block_size].npu()
-            other_tile = other_ids.unsqueeze(dim=1) # B 1 S
+            other_tile = other_ids.unsqueeze(dim=1)  # B 1 S
 
-            mask = this_tile == other_tile # B S S
+            mask = this_tile == other_tile  # B S S
             if kv_block_id > q_block_id:
                 mask = torch.zeros_like(mask)
             elif kv_block_id == q_block_id:
                 mask = torch.tril(mask)
-            
+
             return torch.logical_not(mask).unsqueeze(dim=1).npu()  # B 1 S S
         else:
-            return attn_mask[kv_block_id] if isinstance(attn_mask, list) else None  
-            
+            return attn_mask[kv_block_id] if isinstance(attn_mask, list) else None
 
 
 def ringattn_context_parallel(q, k, v, n, cp_para, softmax_scale=None, attn_mask=None, dropout_p=0.,
                               actual_seq_qlen=None, actual_seq_kvlen=None):
-    AttentionWithCp.block_size = q.shape[0]
-    AttentionWithCp.batch_size = q.shape[1]
-    out = AttentionWithCp.apply(
-        q, k, v, n, cp_para, softmax_scale, attn_mask, dropout_p,
+    attn_with_cp = AttentionWithCp(cp_para)
+    attn_with_cp.block_size = q.shape[0]
+    attn_with_cp.batch_size = q.shape[1]
+    out = attn_with_cp(
+        q, k, v, n, softmax_scale, attn_mask, dropout_p,
         actual_seq_qlen, actual_seq_kvlen
     )
     return out
diff --git a/mindspeed/core/context_parallel/utils.py b/mindspeed/core/context_parallel/utils.py
index 194acd6..a5639ec 100644
--- a/mindspeed/core/context_parallel/utils.py
+++ b/mindspeed/core/context_parallel/utils.py
@@ -9,9 +9,9 @@ from scipy.sparse.linalg import eigsh
 
 from megatron.training import get_args
 from megatron.core.parallel_state import get_context_parallel_global_ranks
-from mindspeed.ops.npu_ring_attention_update import npu_ring_attention_update
+# from mindspeed.ops.npu_ring_attention_update import npu_ring_attention_update
 from mindspeed.core.parallel_state import get_context_parallel_for_hybrid_ring_global_ranks
-from mindspeed.op_builder import AdaptiveCpOpBuilder
+# from mindspeed.op_builder import AdaptiveCpOpBuilder
 
 
 ADAPTIVE_CP_SCHEDULING_INFO = None
@@ -555,5 +555,5 @@ class AdaptiveCpOps:
         return mask_list
 
 
-adaptive_cp_ops = AdaptiveCpOps()
+# adaptive_cp_ops = AdaptiveCpOps()
 
diff --git a/mindspeed/core/models/common/embeddings/rotary_pos_embedding.py b/mindspeed/core/models/common/embeddings/rotary_pos_embedding.py
index 146c856..8359d05 100644
--- a/mindspeed/core/models/common/embeddings/rotary_pos_embedding.py
+++ b/mindspeed/core/models/common/embeddings/rotary_pos_embedding.py
@@ -9,7 +9,7 @@ from megatron.core.models.common.embeddings.rotary_pos_embedding import _rotate_
 from megatron.training import get_args
 from megatron.core import parallel_state
 from mindspeed.utils import get_position_ids
-from mindspeed.ops.npu_rotary_position_embedding import npu_rotary_position_embedding
+# from mindspeed.ops.npu_rotary_position_embedding import npu_rotary_position_embedding
 
 from mindspeed.core.parallel_state import (get_context_parallel_for_hybrid_ulysses_world_size,
                                              get_context_parallel_for_hybrid_ulysses_rank,
@@ -72,7 +72,6 @@ def apply_rotary_pos_emb_bshd(t: Tensor, freqs: Tensor, rotary_interleaved: bool
         t = npu_rotary_position_embedding(t.contiguous(), cos_, sin_, mode).to(t.dtype)
     else:
         t = (t * cos_) + (_rotate_half(t, rotary_interleaved) * sin_)
-
     return torch.cat((t, t_pass), dim=-1)
 
 
diff --git a/mindspeed/core/tensor_parallel/random.py b/mindspeed/core/tensor_parallel/random.py
index 3e96442..fe40d88 100644
--- a/mindspeed/core/tensor_parallel/random.py
+++ b/mindspeed/core/tensor_parallel/random.py
@@ -4,7 +4,6 @@ from typing import List, Union
 import torch
 from torch import _C
 from torch_npu.npu import _lazy_call, device as device_ctx_manager
-from torch.utils.checkpoint import _get_autocast_kwargs
 from megatron.training import get_args
 from megatron.core.tensor_parallel.utils import gather_split_1d_tensor
 from megatron.core.tensor_parallel.random import get_cuda_rng_tracker
diff --git a/mindspeed/core/transformer/moe/token_dispatcher.py b/mindspeed/core/transformer/moe/token_dispatcher.py
index 878c37e..0ff4b59 100644
--- a/mindspeed/core/transformer/moe/token_dispatcher.py
+++ b/mindspeed/core/transformer/moe/token_dispatcher.py
@@ -1,7 +1,7 @@
 # Copyright (c) 2024; NVIDIA CORPORATION. All rights reserved.
 # Copyright (c) 2024, Huawei Technologies Co., Ltd.  All rights reserved.
 import torch
-from torch_npu.utils.collect_env import get_cann_version
+# from torch_npu.utils.collect_env import get_cann_version
 from megatron.training import get_args
 from megatron.core import parallel_state, tensor_parallel
 from megatron.core.transformer.moe.moe_utils import permute, unpermute
@@ -14,11 +14,12 @@ from mindspeed.core.transformer.moe.unpermute_without_activation import Unpermut
 
 
 def is_less_or_equal_rc2_cann_version():
-    cann_starts_with = ('8.0.RC1', '8.0.RC2')
-    cann_all = ('not known', '8.0.T1', '8.0.T2', '8.0.T3', '8.0.T37', '8.0.T5', '8.0.T6', '8.0.T7',
-                '8.0.T8', '8.0.T10', '8.0.T13', '8.0.T16', '8.0.T50', '8.0.T51', '8.0.T52')
-    cann_version = get_cann_version()
-    return cann_version in cann_all or cann_version.startswith(cann_starts_with)
+    # cann_starts_with = ('8.0.RC1', '8.0.RC2')
+    # cann_all = ('not known', '8.0.T1', '8.0.T2', '8.0.T3', '8.0.T37', '8.0.T5', '8.0.T6', '8.0.T7',
+    #             '8.0.T8', '8.0.T10', '8.0.T13', '8.0.T16', '8.0.T50', '8.0.T51', '8.0.T52')
+    # cann_version = get_cann_version()
+    # return cann_version in cann_all or cann_version.startswith(cann_starts_with)
+    return True
 
 
 cann_version_check = is_less_or_equal_rc2_cann_version()
diff --git a/mindspeed/megatron_adaptor.py b/mindspeed/megatron_adaptor.py
index f295aea..bddef60 100644
--- a/mindspeed/megatron_adaptor.py
+++ b/mindspeed/megatron_adaptor.py
@@ -120,14 +120,14 @@ def te_adaptation(aspm):
     aspm.register_patch('importlib.metadata.version', version_wrapper)
     aspm.register_patch('transformer_engine.pytorch.LayerNormLinear', torch.nn.Module, create_dummy=True)
     aspm.register_patch('transformer_engine.pytorch.DotProductAttention', torch.nn.Module, create_dummy=True)
-    aspm.register_patch('transformer_engine.pytorch.Linear', torch.nn.Module, create_dummy=True)
+    # aspm.register_patch('transformer_engine.pytorch.Linear', torch.nn.Module, create_dummy=True)
     aspm.register_patch('flash_attn.flash_attn_interface.flash_attn_unpadded_func', create_dummy=True)
 
 
 def apex_adaptation(aspm):
     from .optimizer.adamw import AdamW
     from .core.fusions.fused_layer_norm import fused_layer_norm_affine
-    from .ops.npu_matmul_add import npu_matmul_add_fp32, npu_matmul_add_fp16
+    # from .ops.npu_matmul_add import npu_matmul_add_fp32, npu_matmul_add_fp16
     aspm.register_patch('apex.optimizers.FusedAdam', AdamW, create_dummy=True)
     aspm.register_patch('amp_C.multi_tensor_l2norm', multi_tensor_l2norm, create_dummy=True)
     aspm.register_patch('amp_C.multi_tensor_scale', multi_tensor_scale, create_dummy=True)
@@ -135,8 +135,8 @@ def apex_adaptation(aspm):
     aspm.register_patch('apex.multi_tensor_apply.multi_tensor_applier', multi_tensor_applier, create_dummy=True)
     aspm.register_patch('apex.normalization.fused_layer_norm.fused_layer_norm_affine', fused_layer_norm_affine,
                         create_dummy=True)
-    aspm.register_patch('fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32', npu_matmul_add_fp32, create_dummy=True)
-    aspm.register_patch('fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16', npu_matmul_add_fp16, create_dummy=True)
+    # aspm.register_patch('fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32', npu_matmul_add_fp32, create_dummy=True)
+    # aspm.register_patch('fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16', npu_matmul_add_fp16, create_dummy=True)
 
 
 def torch_adaptation(aspm):
@@ -313,10 +313,11 @@ def mcore_tensor_parallel_adaptation_l0(aspm):
 
 
 def mcore_tensor_parallel_adaptation_l1(aspm):
-    from .core.tensor_parallel.cross_entropy import vocab_parallel_cross_entropy_forward
+    # from .core.tensor_parallel.cross_entropy import vocab_parallel_cross_entropy_forward
     # use logical negation followed by multiplication to achieve the same effect as setting selected elements to zero
-    aspm.register_patch('megatron.core.tensor_parallel.cross_entropy._VocabParallelCrossEntropy.forward',
-                        vocab_parallel_cross_entropy_forward)
+    # aspm.register_patch('megatron.core.tensor_parallel.cross_entropy._VocabParallelCrossEntropy.forward',
+    #                     vocab_parallel_cross_entropy_forward)
+    pass
 
 
 def mcore_tensor_parallel_adaptation(aspm):
diff --git a/mindspeed/model/transformer.py b/mindspeed/model/transformer.py
index c7fe7b3..fd6f935 100644
--- a/mindspeed/model/transformer.py
+++ b/mindspeed/model/transformer.py
@@ -48,7 +48,7 @@ from mindspeed.core.parallel_state import (get_context_parallel_group_for_hybrid
                                            get_ring_group_for_intra_window_send_recv_overlap)
 from mindspeed.core.fusions.fused_bias_swiglu import fused_swiglu
 from mindspeed.core.tensor_parallel.random import CheckpointWithoutOutput
-from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention
+# from mindspeed.ops.fusion_attention_v2 import npu_fusion_attention
 from mindspeed.core.tensor_parallel.layers import Nd_ParallelLinear
 from mindspeed.core.tensor_parallel.checkpoint_manager import get_pipeline_checkpoint_manager
 from mindspeed.model.alibi_mask import AlibiForFusionAttnSingleton, get_slopes
@@ -547,7 +547,7 @@ def generate_attention_mask():
     args = get_args()
     if args.use_flash_attn and (args.seq_length > 2048 or args.context_parallel_algo in ['megatron_cp_algo', 'hybrid_cp_algo']):
         args.sparse_mode = 2
-        _GLOBAL_ATTN_MASK = torch.triu(torch.ones([2048, 2048], dtype=bool, device=torch.cuda.current_device()), diagonal=1)
+        _GLOBAL_ATTN_MASK = torch.triu(torch.ones([2048, 2048], dtype=torch.bool, device=torch.cuda.current_device()), diagonal=1)
     else:
         args.sparse_mode = 0
         if args.attention_mask_on_cpu:
