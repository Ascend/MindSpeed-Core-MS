training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (3248.52, 3260.46)
    train/valid/test-data-iterators-setup ..........: (961.67, 1047.46)
[before the start of training step] datetime: 2025-05-06 15:54:04
WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version
[W compiler_depend.ts:2573] Warning: Tensor not is not allocated by NPUCachingAllocator, skip eraseStream. (function operator())
Number of parameters in transformer layers in billions:  6.53
Number of parameters in embedding layers in billions: 1.09
Total number of parameters in billions: 7.62
Number of parameters in most loaded shard in billions: 1.0882
Number of parameters in other shards in billions: 0.8157
Theoretical memory footprints: weight and optimizer=18680.38 MB
 [2025-05-06 15:54:26] iteration        1/      10 | consumed samples:          128 | elapsed time per iteration (ms): 22849.3 | throughput per GPU (TFLOP/s/GPU): 299.9 | learning rate: 1.235606E-06 | global batch size:   128 | lm loss: 9.7951211929321289 | loss scale: 1.0 | grad norm: 20.1497642787274636 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 4] (after 1 iterations) memory (MB) | allocated: 12480.07421875 | max allocated: 14014.271484375 | reserved: 13212.0 | max reserved: 14216.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 12480.03173828125 | max allocated: 14014.271484375 | reserved: 13258.0 | max reserved: 14216.0
[Rank 2] (after 1 iterations) memory (MB) | allocated: 12480.03173828125 | max allocated: 14014.271484375 | reserved: 13258.0 | max reserved: 14216.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 16618.2421875 | max allocated: 18688.27587890625 | reserved: 18582.0 | max reserved: 19474.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 16618.2421875 | max allocated: 18688.27587890625 | reserved: 18582.0 | max reserved: 19474.0
[Rank 7] (after 1 iterations) memory (MB) | allocated: 16630.60888671875 | max allocated: 18692.78662109375 | reserved: 17926.0 | max reserved: 18896.0
[Rank 5] (after 1 iterations) memory (MB) | allocated: 12480.07421875 | max allocated: 14014.271484375 | reserved: 13214.0 | max reserved: 14216.0
[Rank 6] (after 1 iterations) memory (MB) | allocated: 16630.60888671875 | max allocated: 18692.78662109375 | reserved: 17926.0 | max reserved: 18896.0
 [2025-05-06 15:54:36] iteration        2/      10 | consumed samples:          256 | elapsed time per iteration (ms): 9344.6 | throughput per GPU (TFLOP/s/GPU): 733.4 | learning rate: 1.166872E-06 | global batch size:   128 | lm loss: 9.7579383850097656 | loss scale: 1.0 | grad norm: 23.7711511490950933 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:54:45] iteration        3/      10 | consumed samples:          384 | elapsed time per iteration (ms): 9133.0 | throughput per GPU (TFLOP/s/GPU): 750.4 | learning rate: 1.048292E-06 | global batch size:   128 | lm loss: 9.8505048751831055 | loss scale: 1.0 | grad norm: 36.9996865748971615 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:54:54] iteration        4/      10 | consumed samples:          512 | elapsed time per iteration (ms): 9006.6 | throughput per GPU (TFLOP/s/GPU): 760.9 | learning rate: 8.921965E-07 | global batch size:   128 | lm loss: 9.6257839202880859 | loss scale: 1.0 | grad norm: 25.7293203331274860 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:03] iteration        5/      10 | consumed samples:          640 | elapsed time per iteration (ms): 8922.6 | throughput per GPU (TFLOP/s/GPU): 768.1 | learning rate: 7.148162E-07 | global batch size:   128 | lm loss: 9.5872354507446289 | loss scale: 1.0 | grad norm: 27.0351616118510805 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:12] iteration        6/      10 | consumed samples:          768 | elapsed time per iteration (ms): 8855.6 | throughput per GPU (TFLOP/s/GPU): 773.9 | learning rate: 5.345956E-07 | global batch size:   128 | lm loss: 9.3978557586669922 | loss scale: 1.0 | grad norm: 24.3384905362274111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:20] iteration        7/      10 | consumed samples:          896 | elapsed time per iteration (ms): 8724.5 | throughput per GPU (TFLOP/s/GPU): 785.5 | learning rate: 3.702742E-07 | global batch size:   128 | lm loss: 9.3909635543823242 | loss scale: 1.0 | grad norm: 21.0742705195147266 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:29] iteration        8/      10 | consumed samples:         1024 | elapsed time per iteration (ms): 8921.3 | throughput per GPU (TFLOP/s/GPU): 768.2 | learning rate: 2.389384E-07 | global batch size:   128 | lm loss: 9.3132848739624023 | loss scale: 1.0 | grad norm: 26.7052780363732580 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:38] iteration        9/      10 | consumed samples:         1152 | elapsed time per iteration (ms): 9018.4 | throughput per GPU (TFLOP/s/GPU): 759.9 | learning rate: 1.542448E-07 | global batch size:   128 | lm loss: 9.3942499160766602 | loss scale: 1.0 | grad norm: 22.5240594021842568 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-05-06 15:55:47] iteration       10/      10 | consumed samples:         1280 | elapsed time per iteration (ms): 9077.5 | throughput per GPU (TFLOP/s/GPU): 755.0 | learning rate: 1.250000E-07 | global batch size:   128 | lm loss: 9.4826784133911133 | loss scale: 1.0 | grad norm: 17.7855950864210612 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-05-06 15:55:47
