training ...
[before the start of training step] datetime: 2025-03-21 18:05:21 
Number of parameters in transformer layers in billions:  26.19
Number of parameters in embedding layers in billions: 1.85
Total number of parameters in billions: 28.04
Number of parameters in most loaded shard in billions: 14.0214
Number of parameters in other shards in billions: 13.0947
Theoretical memory footprints: weight and optimizer=120346.30 MB
 [2025-03-21 18:05:34] iteration        1/      10 | consumed samples:            8 | elapsed time per iteration (ms): 12577.8 | learning rate: 1.000000E-05 | global batch size:     8 | lm loss: 2.5055196285247803 | loss scale: 1.0 | grad norm: 30.5073025200452719 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:35] iteration        2/      10 | consumed samples:           16 | elapsed time per iteration (ms): 606.3 | learning rate: 9.701478E-06 | global batch size:     8 | lm loss: 4.0182037353515625 | loss scale: 1.0 | grad norm: 24.5777919235020157 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:35] iteration        3/      10 | consumed samples:           24 | elapsed time per iteration (ms): 548.2 | learning rate: 8.841920E-06 | global batch size:     8 | lm loss: 6.6559705734252930 | loss scale: 1.0 | grad norm: 26.3775372482527040 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:36] iteration        4/      10 | consumed samples:           32 | elapsed time per iteration (ms): 547.4 | learning rate: 7.525000E-06 | global batch size:     8 | lm loss: 7.3979940414428711 | loss scale: 1.0 | grad norm: 29.8245666203823987 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:36] iteration        5/      10 | consumed samples:           40 | elapsed time per iteration (ms): 535.3 | learning rate: 5.909558E-06 | global batch size:     8 | lm loss: 7.8820257186889648 | loss scale: 1.0 | grad norm: 45.1581563685773872 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:37] iteration        6/      10 | consumed samples:           48 | elapsed time per iteration (ms): 543.2 | learning rate: 4.190442E-06 | global batch size:     8 | lm loss: 6.9983172416687012 | loss scale: 1.0 | grad norm: 64.9748680239632819 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:37] iteration        7/      10 | consumed samples:           56 | elapsed time per iteration (ms): 514.0 | learning rate: 2.575000E-06 | global batch size:     8 | lm loss: 8.9150371551513672 | loss scale: 1.0 | grad norm: 67.4178867048938741 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:38] iteration        8/      10 | consumed samples:           64 | elapsed time per iteration (ms): 525.9 | learning rate: 1.258080E-06 | global batch size:     8 | lm loss: 8.3893833160400391 | loss scale: 1.0 | grad norm: 113.2351324499038441 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:38] iteration        9/      10 | consumed samples:           72 | elapsed time per iteration (ms): 538.5 | learning rate: 3.985215E-07 | global batch size:     8 | lm loss: 10.7215175628662109 | loss scale: 1.0 | grad norm: 39.1557762318268274 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-03-21 18:05:39] iteration       10/      10 | consumed samples:           80 | elapsed time per iteration (ms): 530.8 | learning rate: 1.000000E-07 | global batch size:     8 | lm loss: 10.0541715621948242 | loss scale: 1.0 | grad norm: 68.1348817150455091 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-03-21 18:05:39 