training ...
[before the start of training step] datetime: 2025-04-25 16:46:42
WARNING:megatron.core.models.common.embeddings.rotary_pos_embedding:Setting apply_rope_fusion to false because its implementation is not included in Apex. Try upgrading to the latest version
[W compiler_depend.ts:41] Warning: Warning: kernel [ArgSort] can not support dtype int32 or int64 on AiCore, Now this kernel is running on AiCpu.If you are more concerned about high-performance execution,please cast dtype to float32. (function operator())
Number of parameters in transformer layers in billions:  26.19
Number of parameters in embedding layers in billions: 1.85
Total number of parameters in billions: 28.04
Number of parameters in most loaded shard in billions: 14.0214
Number of parameters in other shards in billions: 13.0947
Theoretical memory footprints: weight and optimizer=120346.30 MB
 [2025-04-25 16:46:58] iteration        1/      10 | consumed samples:            8 | elapsed time per iteration (ms): 15740.8 | learning rate: 9.757730E-06 | global batch size:     8 | lm loss: 13.4216022491455078 | loss scale: 1.0 | grad norm: 11.4072200414918008 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 1 iterations) memory (MB) | allocated: 17975.59765625 | max allocated: 24781.76806640625 | reserved: 28276.0 | max reserved: 28276.0
[Rank 4] (after 1 iterations) memory (MB) | allocated: 31935.140625 | max allocated: 41007.94775390625 | reserved: 42298.0 | max reserved: 42298.0
 [2025-04-25 16:47:01] iteration        2/      10 | consumed samples:           16 | elapsed time per iteration (ms): 3261.9 | learning rate: 9.054634E-06 | global batch size:     8 | lm loss: 13.3996791839599609 | loss scale: 1.0 | grad norm: 10.8172290360941155 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:05] iteration        3/      10 | consumed samples:           24 | elapsed time per iteration (ms): 3223.2 | learning rate: 7.959537E-06 | global batch size:     8 | lm loss: 13.5675144195556641 | loss scale: 1.0 | grad norm: 22.3440501799793338 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:08] iteration        4/      10 | consumed samples:           32 | elapsed time per iteration (ms): 3237.7 | learning rate: 6.579634E-06 | global batch size:     8 | lm loss: 13.3749132156372070 | loss scale: 1.0 | grad norm: 12.7026048016518338 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:11] iteration        5/      10 | consumed samples:           40 | elapsed time per iteration (ms): 3224.2 | learning rate: 5.050000E-06 | global batch size:     8 | lm loss: 13.6682624816894531 | loss scale: 1.0 | grad norm: 12.6189429228244787 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:14] iteration        6/      10 | consumed samples:           48 | elapsed time per iteration (ms): 3259.4 | learning rate: 3.520366E-06 | global batch size:     8 | lm loss: 13.4912853240966797 | loss scale: 1.0 | grad norm: 15.6055236433493629 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:18] iteration        7/      10 | consumed samples:           56 | elapsed time per iteration (ms): 3239.2 | learning rate: 2.140463E-06 | global batch size:     8 | lm loss: 13.4111728668212891 | loss scale: 1.0 | grad norm: 15.3025744129771635 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:21] iteration        8/      10 | consumed samples:           64 | elapsed time per iteration (ms): 3257.1 | learning rate: 1.045366E-06 | global batch size:     8 | lm loss: 13.3682403564453125 | loss scale: 1.0 | grad norm: 14.1228426070615480 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:24] iteration        9/      10 | consumed samples:           72 | elapsed time per iteration (ms): 3275.7 | learning rate: 3.422702E-07 | global batch size:     8 | lm loss: 13.2756052017211914 | loss scale: 1.0 | grad norm: 10.1594756535331712 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-25 16:47:27] iteration       10/      10 | consumed samples:           80 | elapsed time per iteration (ms): 3250.9 | learning rate: 1.000000E-07 | global batch size:     8 | lm loss: 13.1001281738281250 | loss scale: 1.0 | grad norm: 11.8250297013730385 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-04-25 16:47:27
