Number of parameters in transformer layers in billions:  25.78 [2025-04-28 15:49:21] iteration        1/      11 | consumed samples:           16 | elapsed time per iteration (ms): 19814.6 | learning rate: 5.000000E-06 | global batch size:    16 | lm loss: 12.0754003524780273 | loss scale: 1.0 | grad norm: 19.7171962023083367 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |

Number of parameters in embedding layers in billions: 1.85
Total number of parameters in billions: 27.63
Number of parameters in most loaded shard in billions: 6.9079
Number of parameters in other shards in billions: 6.4446
Theoretical memory footprints: weight and optimizer=79054.71 MB
[Rank 5] (after 1 iterations) memory (MB) | allocated: 11679.6484375 | max allocated: 12575.65380859375 | reserved: 13090.0 | max reserved: 13090.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 11651.56298828125 | max allocated: 12547.5654296875 | reserved: 13350.0 | max reserved: 13350.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 11651.56298828125 | max allocated: 12547.5654296875 | reserved: 13350.0 | max reserved: 13350.0
[Rank 4] (after 1 iterations) memory (MB) | allocated: 11679.6484375 | max allocated: 12575.65380859375 | reserved: 13090.0 | max reserved: 13090.0
 [2025-04-28 15:49:24] iteration        2/      11 | consumed samples:           32 | elapsed time per iteration (ms): 3663.8 | learning rate: 4.902113E-06 | global batch size:    16 | lm loss: 12.1699361801147461 | loss scale: 1.0 | grad norm: 17.7069087214662311 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:28] iteration        3/      11 | consumed samples:           48 | elapsed time per iteration (ms): 3620.9 | learning rate: 4.618034E-06 | global batch size:    16 | lm loss: 12.0397014617919922 | loss scale: 1.0 | grad norm: 15.3973619926400858 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:31] iteration        4/      11 | consumed samples:           64 | elapsed time per iteration (ms): 3650.6 | learning rate: 4.175571E-06 | global batch size:    16 | lm loss: 11.5350246429443359 | loss scale: 1.0 | grad norm: 15.8681698020348581 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:35] iteration        5/      11 | consumed samples:           80 | elapsed time per iteration (ms): 3615.4 | learning rate: 3.618034E-06 | global batch size:    16 | lm loss: 11.6795530319213867 | loss scale: 1.0 | grad norm: 13.9077046865774303 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:39] iteration        6/      11 | consumed samples:           96 | elapsed time per iteration (ms): 3663.0 | learning rate: 3.000000E-06 | global batch size:    16 | lm loss: 11.1276912689208984 | loss scale: 1.0 | grad norm: 21.3548735512446548 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:42] iteration        7/      11 | consumed samples:          112 | elapsed time per iteration (ms): 3623.2 | learning rate: 2.381966E-06 | global batch size:    16 | lm loss: 11.1152887344360352 | loss scale: 1.0 | grad norm: 15.4521171156286066 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:46] iteration        8/      11 | consumed samples:          128 | elapsed time per iteration (ms): 3651.7 | learning rate: 1.824429E-06 | global batch size:    16 | lm loss: 10.6487312316894531 | loss scale: 1.0 | grad norm: 19.5008472845428820 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:50] iteration        9/      11 | consumed samples:          144 | elapsed time per iteration (ms): 3670.7 | learning rate: 1.381966E-06 | global batch size:    16 | lm loss: 10.8181762695312500 | loss scale: 1.0 | grad norm: 14.7328314999231207 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:53] iteration       10/      11 | consumed samples:          160 | elapsed time per iteration (ms): 3673.4 | learning rate: 1.097887E-06 | global batch size:    16 | lm loss: 10.7660694122314453 | loss scale: 1.0 | grad norm: 11.1571546770113113 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-04-28 15:49:57] iteration       11/      11 | consumed samples:          176 | elapsed time per iteration (ms): 3703.4 | learning rate: 1.000000E-06 | global batch size:    16 | lm loss: 10.6332979202270508 | loss scale: 1.0 | grad norm: 10.7624934808974952 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
[after training is done] datetime: 2025-04-28 15:49:57
