model_parallel:
  # model parallelism
  context_parallel_size: 8
  context_parallel_algo: "megatron_cp_algo" # Use Ring Attention

language_model:
  # model architecture
  num_attention_heads: 32

# training
micro_batch_size: 1

# data
max_position_embeddings: 8192
seq_length: 8192