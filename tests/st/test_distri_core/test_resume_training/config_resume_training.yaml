model_parallel:
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 2
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: True
  pipeline_dtype: "float32"

language_model:
  num_layers: 2
  num_attention_heads: 16
  hidden_size: 32
  ffn_hidden_size: 128
  num_query_groups: 8
  hidden_dropout: 0.0
  attention_dropout: 0.0
  init_method: 'normal'
  add_qkv_bias: False
  mask_func_type: "attn_mask_fill" # need add
  normalization: "FusedRMSNorm"
  layernorm_epsilon: 0.00001  # norm_epsilon
  gated_linear_unit: False
  activation_func: "silu"  # hidden_act

  num_moe_experts: 1
  moe_router_topk: 2
  moe_token_dispatcher_type: 'alltoall'
  moe_z_loss_coeff: 1.e-3
  moe_aux_loss_coeff: 1.e-2
  moe_router_load_balancing_type: 'aux_loss' # ['none', 'aux_loss']
  moe_expert_capacity_factor: -1
  moe_token_drop_policy: null
  moe_pad_expert_input_to_capacity: False

  masked_softmax_fusion: False
  bias_dropout_fusion: False
  # Must add
  clone_scatter_output_in_embedding: False
  attention_softmax_in_fp32: True
  apply_rope_fusion: False
  add_bias_linear: False

# data
seq_length: 4
vocab_size: 64
padded_vocab_size: 64
eod_id: -100
seed: 1921
pad_token: 0
save: "./output"
data_path: 'data'
group_query_attention: True

# training
position_embedding_type: 'rope'
max_position_embeddings: 4
epochs: 1
train_iters: 10
untie_embeddings_and_output_weights: True
global_batch_size: 1
micro_batch_size: 1
clip_grad: 0.0
transformer_impl: "local"
use_post_norm: False
repeat_kv_outside: True             # if False, repeat kv after rotary pos emb.

# optimizer
optimizer: "mint.AdamW"
lr_decay_style: "WSD"
weight_decay_incr_style: "cosine"
lr: 0.0009
lr_decay_iters: 7000
lr_wsd_decay_iters: 7000
lr_warmup_fraction: 0.0005
lr_warmup_iters: 0
lr_decay_samples: null
lr_wsd_decay_samples: 7000
lr_warmup_samples: 0
lr_warmup_init: 0.0009
min_lr: 0.00001
start_weight_decay: 0.00001
end_weight_decay: 0.0009
use_checkpoint_opt_param_scheduler: True
override_opt_param_scheduler: False
lr_wsd_decay_style: "exponential"

# distributed arguments
delay_grad_reduce: False
wrap_with_ddp: False

# loss arguments
initial_loss_scale: 1  # loss_scale_value
hysteresis: 2   # loss_scale_factor
loss_scale_window: 1000
loss_reduction: "mean"
fp16_lm_cross_entropy: False # need add
loss_scale: 1
loss_func_kwargs:
  loss_func_type: "CrossEntropyLoss"

# logging
log_interval: 1
save_interval: 5

# custom arguments
model_customize_staged: False
dist_ckpt_format: "ckpt"

#resume training
resume_training: False
crc_check: False
ckpt_format: "ckpt"
prefix: "network"
load: ""
enable_compile_cache: False
compile_cache_path: "./output/compile_cache"
keep_checkpoint_max: 3
no_load_optim: False
no_load_rng: True
new_dataset: False
profile: False
profile_save_path: ""
profile_step_start: 2
profile_step_end: 4
profile_level: level0
profile_with_stack: True
profile_memory: False
profile_framework: all
profile_communication: True
profile_parallel_strategy: False
profile_aicore_metrics: 0
profile_l2_cache: False
profile_hbm_ddr: False
profile_pcie: False
profile_data_process: False
profile_data_simplification: False
profile_op_time: False
profile_offline_analyse: False
profile_dynamic_profiler_config_path: ""

