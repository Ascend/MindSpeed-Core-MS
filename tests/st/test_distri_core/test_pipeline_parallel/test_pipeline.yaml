model_parallel:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 4
  context_parallel_size: 1
  expert_model_parallel_size: 1
  virtual_pipeline_model_parallel_size: null
  sequence_parallel: False
  pipeline_dtype: "float32"
  params_dtype: "float32"
  compute_dtype: "float32"
  variable_seq_lengths: False

language_model:
  num_layers: 8
  num_attention_heads: 1
  num_query_groups: 1
  hidden_size: 128
  ffn_hidden_size: 1
  hidden_dropout: 0.0
  attention_dropout: 0.0
  mask_func_type: "attn_mask_add" # need add

  # Must add
  clone_scatter_output_in_embedding: False
  attention_softmax_in_fp32: True
  apply_rope_fusion: False
  add_bias_linear: False
  num_moe_experts: 1

# data
seq_length: 64
vocab_size: 256

# training
epochs: 1
train_iters: 2
untie_embeddings_and_output_weights: True
global_batch_size: 16
micro_batch_size: 2
max_position_embeddings: 64
shape_order: "BSH"
pre_tockens: 2147483647
next_tockens: 2147483647

# loss arguments
initial_loss_scale: 1  # loss_scale_value
hysteresis: 2   # loss_scale_factor
loss_scale_window: 1000
fp16_lm_cross_entropy: False # need add

# distributed arguments
wrap_with_ddp: False

# logging
log_interval: 1

# custom arguments
model_customize_staged: False