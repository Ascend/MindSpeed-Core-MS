model_parallel:
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: False
  pipeline_dtype: "float32"
  params_dtype: "float32"
  compute_dtype: "float32"
  softmax_compute_dtype: "float32" # need add
  zero_level: z3
  finalize_model_grads_func: null

language_model:
  num_layers: 1
  num_attention_heads: 2
  hidden_size: 16
  ffn_hidden_size: 64
  hidden_dropout: 0.0
  attention_dropout: 0.0
  add_out_proj_bias: False
  add_mlp_bias: True
  normalization: "FusedRMSNorm"
  layernorm_epsilon: 1.e-5  # norm_epsilon
  activation_func: "gelu"  # hidden_act
  masked_softmax_fusion: False
  bias_dropout_fusion: False

  # Must add
  clone_scatter_output_in_embedding: False
  attention_softmax_in_fp32: True
  apply_rope_fusion: False
  add_bias_linear: False

# data
seq_length: 16
vocab_size: 1
eod_id: -100
global_batch_size: 1
micro_batch_size: 1
batch_size: 1

# training
position_embedding_type: 'rope'
max_position_embeddings: 4096
epochs: 1
train_iters: 2
untie_embeddings_and_output_weights: False
clip_grad: 0.0
save: "./output"
save_interval: 1
optimizer: "AdamW"
adam_beta1: 0.9
adam_beta2: 0.999
eps: 1.e-6
weight_decay: 0.0
lr: 1.e-3

# distributed arguments
delay_grad_reduce: False

# loss arguments
initial_loss_scale: 1  # loss_scale_value
hysteresis: 2   # loss_scale_factor
loss_scale_window: 1000
loss_reduction: "mean"
fp16_lm_cross_entropy: False # need add

# pangu arguments
use_query_layer: False
use_visual_encoder: False

# logging
log_interval: 1

# custom arguments
model_customize_staged: False