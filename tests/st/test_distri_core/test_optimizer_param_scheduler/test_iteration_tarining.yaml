model_parallel:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 4
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: False
  pipeline_dtype: "float32"
  compute_dtype: "float32"

language_model:
  num_layers: 8
  hidden_size: 128
  num_attention_heads: 1
  ffn_hidden_size: 1
  num_moe_experts: 1

# data
seq_length: 64
vocab_size: 256
pad_token_id: -100

# trainging
global_batch_size: 16
micro_batch_size: 2
max_position_embeddings: 64

# optimizer
lr: 3.e-4
lr_decay_iters: 1320000
lr_wsd_decay_iters: 0
lr_warmup_fraction: null
lr_warmup_iters: 2000
lr_decay_samples: null
lr_wsd_decay_samples: null
lr_warmup_samples: 0
lr_warmup_init: 0.0
min_lr: 3.e-5
lr_decay_style: "cosine"
weight_decay: 0.1
start_weight_decay: null
end_weight_decay: null
weight_decay_incr_style: "constant"
use_checkpoint_opt_param_scheduler: True
override_opt_param_scheduler: False

# loss arguments
epochs: 10
train_iters: 100
log_interval: 1
initial_loss_scale: 1
hysteresis: 2
loss_scale_window: 1000
save_interval: 1

model_customize_staged: False
overlap_grad_reduce: False
untie_embeddings_and_output_weights: True
save: "./output"