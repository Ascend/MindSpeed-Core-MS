model_parallel:
  tensor_model_parallel_size: 2           # tensor_parallel
  pipeline_model_parallel_size: 1         # pipeline_stage
  context_parallel_size: 1                # context_parallel
  expert_model_parallel_size: 1           # expert_parallel
  virtual_pipeline_model_parallel_size: null
  sequence_parallel: False
  pipeline_dtype: "float32"               # recv_dtype
  params_dtype: "bfloat16"
  compute_dtype: "bfloat16"
  softmax_compute_dtype: "bfloat16"

language_model:
  num_layers: 1
  hidden_size: 6144
  num_attention_heads: 48
  attention_softmax_in_fp32: False
  mask_func_type: "attn_mask_fill"
  hidden_dropout: 0.0
  attention_dropout: 0.0
  activation_func: "swiglu"               # hidden_act
  normalization: "FusedRMSNorm"
  layernorm_epsilon: 1.e-5
  apply_residual_connection_post_layernorm: False   # apply_residual_connection_post_norm
  add_qkv_bias: False                     # qkv_has_bias
  add_bias_linear: False
  ffn_hidden_size: 1
  fa_config:
    input_layout: "SBH"

  num_query_groups: 8

use_flash_attn: True
group_query_attention: True
position_embedding_type: 'rope'       # default "learned_absolute"

repeat_kv_outside: False              # if False, repeat kv after rotary pos emb.
# distributed arguments
overlap_grad_reduce: False
# custom arguments
model_customize_staged: False

# training
epochs: 1
train_iters: 2
micro_batch_size: 1
global_batch_size: 8
untie_embeddings_and_output_weights: True

log_interval: 1
initial_loss_scale: 1   # loss_scale_value
hysteresis: 2           # loss_scale_factor
loss_scale_window: 1000
loss_reduction: "mean"
# data
seq_length: 4096
max_position_embeddings: 4096
vocab_size: 165664
data_layout: SBH

pad_token_id: -100
