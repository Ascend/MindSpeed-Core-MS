model_parallel:
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 1
  context_parallel_size: 1
  expert_model_parallel_size: 1
  sequence_parallel: False
  pipeline_dtype: "float32"

language_model:
  num_layers: 2
  num_attention_heads: 8
  num_query_groups: null
  hidden_size: 64
  ffn_hidden_size: 256
  hidden_dropout: 0.0
  attention_dropout: 0.0
  init_method: 'normal'
  add_qkv_bias: True
  mask_func_type: "attn_mask_add" # need add
  normalization: "FusedRMSNorm"
  layernorm_epsilon: 0.00001  # norm_epsilon
  activation_func: "gelu"  # hidden_act

  masked_softmax_fusion: False
  bias_dropout_fusion: False

  # Must add
  clone_scatter_output_in_embedding: False
  attention_softmax_in_fp32: True
  apply_rope_fusion: False
  add_bias_linear: True

  # LoRA
  use_lora: True
  lora_target_cells: [
    {
      'target_cells': [
        '.*.out_proj',
        '.*.mapping',
        '.*.projection',
        'transformer.layers.0.attention.qkv_proj'
      ]
    },
    {
      'cell': 'transformer.layers.0.attention.qkv_proj',
      'rank': 4,
      'alpha': 16
    }
  ]

# data
seq_length: 16
vocab_size: 50304
eod_id: -100

# training
position_embedding_type: 'rope'
max_position_embeddings: 16
epochs: 1
train_iters: 1
untie_embeddings_and_output_weights: False
global_batch_size: 1
micro_batch_size: 1
clip_grad: 0.0
transformer_impl: "local"
use_post_norm: False
group_query_attention: False

# optimizer
optimizer: "mint.AdamW"
lr_decay_style: "constant"
weight_decay_incr_style: "constant"

# distributed arguments
delay_grad_reduce: False
wrap_with_ddp: False

# loss arguments
initial_loss_scale: 1  # loss_scale_value
hysteresis: 2   # loss_scale_factor
loss_scale_window: 1000
loss_reduction: "mean"
fp16_lm_cross_entropy: False # need add

# logging
log_interval: 1

# custom arguments
model_customize_staged: False
